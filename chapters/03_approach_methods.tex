% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

%% TODOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO

% â€¢ Describing different methods: my method was kind of vorgegeben, nur warum nicht amac und gp
% â€¢ Indicating a specific method:
% â€¢ Giving reasons why a particular method was adopted
% â€¢ Indicating sample size and characteristics
% â€¢ Indicating reasons for sample characteristics
% â€¢ Describing the process: Indicating problems or limitations

\chapter{Method to hide cache misses with coroutines}\label{chapter:MethodCoroutinesHidingCacheMisses}

This chapter describes the history of different methods that were used to implement coroutines for hiding cache misses in data structures.
It also explains why coroutines and cache prefetching were chosen as the method to hide cache misses in pointer chasing data structures
and which method is superior to others in terms of performance and implementation complexity.
% However, software prefetching does not work if the data addresses are not known in advance. To the operations in pointerchasing applications, e.g., traversing skip lists, looking up hash tables and searching trees, the addresses of next nodes are not known
% until current nodes are processed. The time between when the addresses are known and the data are needed is much less than the
% desirable prefetching distance. We call the accesses with such a pattern as immediate memory accesses. Such accesses can only benefit
% from the software prefetching by taking inter-task parallelism [21].
% Specifically, after a task issues memory prefetching, it will not continue because the prefetched data is not ready in time. Instead, the
% execution switches to other tasks, and then goes back to process the
% prefetched data after a while.


\section{Why using Coroutines combined with Software Prefetching?}
This thesis focuses on using coroutines and cache prefetching to hide cache misses in pointer chasing data structures.
The reason for this choice of using coroutines  (see \autoref{sec:CppCoroutines}) and software prefeteching ()\autoref{subsec:SoftwarePrefetching}) is,
that coroutines offer an alternative that combines the benefits of AMAC and GP approaches:
they enable interleaved execution like AMAC while preserving the synchronous programming model like GP,
but with compiler-generated context-switching code rather than manual state machine implementation.
Suspension points like co\_await or co\_yield can be simply added after prefetch operations,
and the compiler handles the the efficient state management and context switching by transforming the coroutine function into a state machine.
This significantly reduces the implementation complexity compared to AMAC, and also to GP, as the coroutine syntax allows writing code in a straightforward sequential manner.

As Coroutines are already widely used in asynchronous programming to handle I/O-bound tasks without blocking threads successfully in C++ applications,
this thesis explores their potential in hiding cache misses in pointer-chasing data structures across machines and compilers.

For the pretch operation itself, the x86 intrinsic \_mm\_prefetch is used to issue prefetch instructions to the CPU.
\begin{lstlisting}[language=C, frame=single, caption={Prefetch macro}, label={lst:prefetch_macro}]
#define PREFETCH(addr) _mm_prefetch(addr, _MM_HINT_NTA)
\end{lstlisting}


% why different nr of Coroutines? because of LFB size, which provides best performance
% TODOOO: explain why using prefetchT0YPE_T0 vs T1 or T2
% more on why this thesis really focusses on coorutines? and this specific use case of hiding cache misses with the new c++20 coroutines?
% comparison to amac and bulk prefetching



% Pointer chasing data structures, such as hash tables with chaining or linear probing, and tree-based structures like B-trees or binary search trees,
% often suffer from cache misses due to their non-contiguous memory access patterns.
% When traversing these data structures, the CPU frequently encounters cache misses, leading to significant performance degradation as it waits for data to be fetched from main memory.
% To mitigate this issue, software prefetching is employed to proactively load data into the CPU cache before it is actually needed.
% By issuing prefetch instructions ahead of time, the CPU can overlap memory access latency with computation,
% thereby reducing the time spent waiting for data to arrive.
% Coroutines provide a powerful mechanism to implement this prefetching strategy effectively.

% With the recent standardization in C++20 [19], coroutines greatly ease the implementation of software prefetching. Coroutines [38] are functions that can suspend voluntarily and be resumed later.
% Functions that involve pointer chasing can be written as coroutines which are executed (interleaved) in batches.
% Before dereferencing a pointer in coroutine ð‘¡1, the thread issues a prefetch followed by a suspend to pause ð‘¡1 and switches to another coroutine ð‘¡2,
% overlapping data fetching in ð‘¡1 and computation in ð‘¡2. Compared to earlier approaches [5, 26], coroutines only require
% prefetch/suspend be inserted into sequential code, greatly simplifying implementation while delivering high performance, as the switching overhead can be cheaper than a last-level cache miss [21].
% However, adopting software prefetching remains challenging. First, existing approaches typically use intra-transaction batching which mandates multi-key interfaces that can break backward compatibility.
% For example, in Figure 1(b) an application1 uses multi_get to retrieve a batch of records at once in a transaction. Cache misses caused by probing k1 (k2) in a tree are hidden behind the computation part of probing k2 (k1). While intratransaction batching is a natural fit for some operators (e.g., INpredicate queries [44, 45]), it is not always directly applicable.
% Changing the application is not always feasible and may not achieve the desired improvement as depending requests need to be issued in
% The â€œapplicationâ€ may be another database system component or an end-user application that uses the record access interfaces provided by the database engine.
% separate batches, limiting interleaving opportunities. Short (or even single-record) transactions also cannot benefit much due to the lack of interleaving opportunity. It would be desirable to allow batching operations across transactions, i.e., inter-transaction batching.
% Second, prior work provided only piece-wise solutions, focusing on optimizing individual database operations (e.g., index traversal [21] and hash join [5, 44]). Despite the significant improvement (e.g., up to 3Ã— faster for tree probing [21]), it was not clear how much
% overall improvement one can expect when these techniques are applied in a full database engine that involves various components.
% Overall, these issues lead to two key questions:
% â€¢ How should a database engine adopt coroutine-based software prefetching, preferably without requiring application changes?
% â€¢ How much end-to-end benefit can software prefetching bring to a database engine under realistic workloads?


% \subsection{Different Methods using Coroutines}\label{subsec:DifferentMethodsUsingCoroutines}
\subsection{Different coroutine approaches and their trade-offs}\label{subsec:}
% Citation test~\parencite{latex}.

During the development of coroutine-based data structure lookups, the method of implementing coroutines evolved through several iterations.
The following paragraphs describe the different approaches taken, their trade-offs, and the reasoning behind the final chosen method.

The first approach used is the most straightforward one, which is creating a new coroutine instance for each lookup value/operation.
This means, each time a lookup is performed, a new coroutine is instantiated, executed until the first suspension point (after prefetching),
and then destroyed once the lookup is complete. This involves significant overhead due to the frequent creation and destruction of coroutine state machines,
which includes allocating stack space and managing the coroutine's context, including saving to the heap as explained in \autoref{sec:CppCoroutines}.
Despite being the method most easiest to understand,
having to do all this overhead implicates that this method is inferior in performance compared to other methods.

This major drawback leads to the second approach, which is to create a fixed number of coroutines at the beginning of the lookup process
and to re-use coroutines so the creation overhead is minimal and the lookup process can benefit from interleaved execution of multiple lookups.
With this second approach, the implemented coroutine function for the lookup could now leverage co\_yield to return found values back to the caller,
while the caller is responsible for resuming the coroutine until all lookups are processed.
Furthermore, the caller is responsible for distinguishing between three states:
\begin{itemize}
    \item 1) the lookup is not yet finished (the coroutine yielded without finding the value yet),
    \item 2) the lookup found the value (the coroutine yielded with a found value), and
    \item 3) the lookup did not find the value (the coroutine finished without yielding a value).
\end{itemize}

Looking at this, it is not a binary choice between found vs. not found, but a ternary choice that the caller has to handle.
To distinguish between these three states, as solution, the coroutine function could yield an optional value type,
where a value indicates that the lookup found the value, and a nullopt indicates that the lookup is not yet finished.
However, this requires eihter a fundamental change to the data structures implementation, having a nullopt value type,
or constructing a wrapper type that can hold either a valid value or a nullopt making the lookup procedure more complex.

This complexity and the need to handle three different states led to the final approach, which is to pass a pointer/reference to the result value into the coroutine function.
The coroutine function then writes the found value into the provided pointer/reference when the lookup is successful.
If the lookup is not yet finished, the coroutine simply suspends without modifying the pointer/reference.

This final approach is from this point on used for all coroutine implementations in different data structures throughout this thesis.

% Verlauf von meiner Coroutines usage:
% erst fÃ¼r jeden part eine coroutine erstellt und wiedre gelÃ¶scht -> ineffizient, weil erstellen von coroutinen viel Zeit und stack storage braucht.
% Dann mit N (e.g. =5,10,...) wiederverwendbaren Coroutinen, die erst zerstÃ¶rt werden, wenn alles durch, und imt co_yield und co_await zu suchenden Key gegeben
% und coroutine yielded (nicht mehr returned) Value zurÃ¼ck, wenn gefunden! (Problem: Unterscheidung zwischen noch nicht gefunden in HT, weil noch weiter in chaining durchtraversieren oder in linear
% weiter durchspringen muss, vs schon gefunden vs. nicht in Hashtable da (also nicht auffindbar, da Key nicht existiert)).
% Handhabarkeit schwierig, da wenn nicht gefunden, dann nullpointer zurÃ¼ck, wenn gefunden dann obviously den Value, aber was wenn "noch" nicht gefunden, da in der CHain/Linear HT
% noch weiter traversiert werden muss (aber ich das mit prefetches ebenfalls optimieren will und da dann nicht synchron langsam vorgehen will) ->
% unterschied zwischen nullpointe,r optional value/definierter Value der dafÃ¼r steht dass noch nicht gefunden UND Value (gefunden).

% Finaler Versuch, mit wiederverwendbaren Coroutinen und niht mehr co_yield aber mit pointer, wo dann bei Erfolg in Pointer zurÃ¼ckgeschrieben wird.
% -> Vorteil von wiederverwendbaren Coroutinen und einfacher handhabbar mit co_await auch wenn nicht gefunden oder "noch" nicht.


\subsection{Scheduler for multiple coroutines}\label{subsec:SchedulerForCoroutines}
As in previous section \ref{subsec:DifferentMethodsUsingCoroutines}, the final approach uses a fixed number of coroutines that are created once at the beginning of the lookup process and re-used for multiple lookups.
To manage the execution of these multiple coroutines,
a simple scheduler is implemented that resumes each coroutine in a round-robin fashion until all lookups are processed, illustrated in \autoref{lst:scheduler}.

\begin{lstlisting}[language=C++, caption={Scheduler for lookup}, label={lst:scheduler}, numbers=left]
template<const size_t numCoroutines>
std::vector<typename resultType>
lookupEntries(datastructure ds, lookupsVector lookups) noexcept {
    resultType result;
    std::array<Silent, numCoroutines> coroutines =
        ([&ht, &lookups, &result]<size_t... Is>(std::index_sequence<Is...>) {
            return std::array{lookup<numCoroutines>(ht, lookups, result, Is)...};
        })(std::make_index_sequence<numCoroutines>{});
    while (true) {
        size_t activeCoroutines = 0;
        for (auto &c: coroutines) {
            if (!c.h_.done()) {
                activeCoroutines++;
                c.h_.resume();
            }
        }
        if (activeCoroutines == 0) { break; }
    }
    return result;
}
\end{lstlisting}

Comparing this independently developed and implemented scheduler to the one used in CoroBase \parencite{corobase} (see \autoref{lst:corobase_scheduler}),
both schedulers follow a similar round-robin approach to resume coroutines until all tasks are completed.
However, the CoroBase scheduler is presented in a more abstract pseudocode format, while the implemented scheduler in \autoref{lst:scheduler} is written in concrete C++ code,
showing how to create and manage an array of coroutine instances using C++ templates and standard library features.
Especially, using C++ template metaprogramming to create an array of coroutines at compile time
and the handling of checking if all coroutines are finished are to be highlighted.
By counting active coroutines in each iteration, and breaking the while loop only when none are left,
the implemented scheduler reduces unnecessary checks and improves branch prediction as much as possible as only
in the one last case where all coroutines are done, the loop breaks and the branch prediction is potentially wrong.

\begin{lstlisting}[language=Python, caption={CoroBase Scheduler for lookup}, label={lst:corobase_scheduler}]
for i = 0 to batch_size - 1:
    coroutine[i] = foo(...);
while any(coroutine_promises, x: not x.done()):
    for i = 0 to batch_size - 1:
        if not coroutine[i].done():
            coroutine[i].resume()
\end{lstlisting}


% ========= Using Coroutines in data structures =============
\section{Using Coroutines in different data structures}\label{sec:UsingCoroutinesInDifferentDataStructures}

This section describes the integration of coroutines into different pointer-chasing data structures by using aforementioned scheduler.
It explains how coroutines are integrated into chaining hash tables, linear probing hash tables, and B+ trees.
For each data structure, the implementation of the coroutine-based lookup function is presented,
pointing out details and key aspects of the implementation.

% ----------- Hashtable -----------
\subsection{Coroutines in chaining Hashtable}\label{sec:CoroutinesInHashtable}
The first datastructure where coroutines are integrated is a chaining hashtable.
The chaining hashtable consists of an array of buckets,
where each bucket contains a linked list of entries that hash to the same index.
When performing a lookup, the hash index is computed from the key, and the corresponding bucket is accessed.
The linked list in the bucket is then traversed to find the entry with the matching key.
The size of the hashtable is chosen to be a power of two to allow efficient computation of the hash index using a bitwise AND operation with a mask
and is the next power of two of the handed-over size during construction.
So for all hashtables constructed the load factor is 100\% to reflect a bad hastable scenario with many collisions.
The hash function used is the MurmurHash3 algorithm:% \parencite{murmurhash3}.

The coroutine-based lookup function for the chaining hashtable is shown in \autoref{lst:coroutine_ht_lookup}.
For schedulign multiple coroutines, the scheduler from \autoref{lst:scheduler} is used.
Each coroutine processes a fixed-size subset of the lookup keys, determined by the offset parameter and the total number of coroutines,
meaning that each coroutine from start has its own index to start processing keys from and does not consider if other coroutines
are already finished with their subset of keys that they continue to process other keys from other coroutines.
The Coroutines are eagerly started and for each key, the hash index is computed, and a prefetch instruction is issued for the
corresponding bucket to bring the first entry into the cache. Directly after the prefetch, the coroutine suspends itself using co\_await.
When the coroutine is resumed, it reads the prefetched entry and eventually traverses the linked list in the bucket to find the matching key
until either the key is found or the end of the list is reached.
If a matching key for the to searching key is found, the corresponding entry is added to the results vector,
a pointer/reference passed into the coroutine function from the calling scheduler.
The prefetches are always directly followed by a suspension of the coroutine to allow other coroutines to run while the data is being fetched into the cache.
If one coroutine is finished it returns with co\_return and the scheduler will not resume it anymore.

\begin{lstlisting}[language=C, frame=single, caption={coroutine lookup in chaining hashtable}, label={lst:coroutine_ht_lookup}]
template<const size_t numCoroutines>
Task lookup(const ht_primary_key::Hashtable &ht, const auto &lookups, auto &results,
                const size_t offset) noexcept {
    auto idx = offset; size_t key; uint64_t hashIdx;

    for (; idx < lookups.size(); idx += numCoroutines) {
        key = lookups[idx];
        hashIdx = hashKey(key) & ht.mask;
        PREFETCH(&(ht.ht[hashIdx]));    // Initial prefetch of 1st value
        co_await std::suspend_always{}; // Suspend coroutine

        auto result = ht.ht[hashIdx]; // Read previously cached value
        while (!((result == nullptr) || (result->key == key))) {
            PREFETCH(result->next); // Prefetch next entry in linked list
            co_await std::suspend_always{};
            result = result->next;
        }
        if (result != nullptr) results.push_back(result);
    }
    co_return;
}
\end{lstlisting}


% --------- Linear Probing Hashtable ------------
\subsection{Coroutines in linear probing hashtable}\label{sec:CoroutinesInLinearProbingHashtable}

Linear hashtable uses open addressing with linear probing to resolve collisions.
When a collision occurs (i.e., when two keys hash to the same index), the hashtable probes the next available slot in a linear manner
until an empty slot is found or the desired key is located.
For the same key, chaining is used to store multiple entries in a linked list at the same slot.

Hashtables doubles in size when the load factor exceeds 70\% to maintain efficient lookup performance.




% The coroutine-based lookup function for the linear probing hashtable is shown in \autoref{lst:linear_probing_ht_lookup_coro}.

\begin{lstlisting}[language=C, frame=single, caption={linear probing hashtable coroutine lookup}, label={lst:linear_probing_ht_lookup_coro}]
template<const size_t numCoroutines>
Task lookup_ht_entry_t(const LinearProbingHashTable<uint64_t, uint64_t> &ht,
                            const auto &lookups, auto &results, const size_t offset) noexcept {
    auto idx = offset; size_t key; uint64_t hashIdx; uint64_t hash;
    LinearProbingHashTable<uint64_t, uint64_t>::EntryData *data = nullptr;

    for (; idx < lookups.size(); idx += numCoroutines) {
        key = lookups[idx];
        hash = ht.ComputeHash(key);
        uint64_t slot = hash & ht.bitmask;
        uint64_t salt = ht.ExtractSalt(hash);

        // Find SLOT
        ht_entry_t entry;
        while (true) {
            PREFETCH(&(ht.entries[slot]));  // Initial prefetch of the first value
            co_await std::suspend_always{}; // Suspend the coroutine until resumed
            entry = ht.entries[slot];       // Read the previously cached value
            if (!entry.IsOccupied()) {
                // Empty slot found
                break;
            }
            if (ht.use_salts) {
                if (entry.GetSalt() == salt) {
                    data = reinterpret_cast<
                            LinearProbingHashTable<uint64_t, uint64_t>::EntryData *>(
                            entry.GetPointer());
                    if (data && ht.key_equal(data->key, key)) {
                        entry = ht.entries[slot]; // Found matching key
                        break;
                    }
                }
            } else {
                data = reinterpret_cast<LinearProbingHashTable<
                        uint64_t, uint64_t>::EntryData *>(entry.GetPointer());
                if (data && ht.key_equal(data->key, key)) {
                    entry = ht.entries[slot]; // Found matching key
                    break;
                }
            }
            // Continue linear probing
            data = nullptr;
            lIncrementAndWrap(slot, ht.bitmask);
        }

        if(data == nullptr) { continue; } // Not found
        // Found an entry, retrieve all values in the chain
        LinearProbingHashTable<uint64_t, uint64_t>::EntryData *current = data;
        while (current) {
            results.push_back(current);
            PREFETCH(current->next);
            co_await std::suspend_always();
            current = current->next;
        }
    }
    co_return;
}
\end{lstlisting}



% --------- B+ Tree ------------
\subsection{Coroutines in B+ Tree}\label{sec:CoroutinesInBPlusTree}
The B+ tree base lookup implementation traverses the tree from root to leaf to locate keys in the data structure.
The tree is partitioned into internal nodes, which contain routing information, and leaf nodes,
which store the actual key-value pairs. Each node contains a pointer to its parent node, an array of keys, the current number of keys,
and either an array of child pointers (for internal nodes) or an array of values (for leaf nodes).
For a given lookup key, the algorithm starts at the root node and iteratively descends through the tree.
At each internal node, the keys are searched to determine which child subtree should be examined next.
The current implementation uses a sequential linear scan where the comparison \texttt{key >= node->keys[i]} identifies the correct position
in the sorted key array of the internal node and thus the appropriate child pointer to follow.
This binary tree property ensures that all keys smaller than the branching key are in the left subtree,
while keys greater than or equal to it are in the right subtree.
For larger nodes, the additionally implemented binary search could reduce the number of comparisons from $O(n)$ to $O(\log n)$,
though the linear scan is often sufficient for typical B+ tree node sizes being by default $128$ due to cache efficiency.
When reaching a leaf node, the algorithm performs a final linear search of the keys to find a match.
If a matching key is found, the corresponding value is returned else the lookup concludes without a result.

The coroutine-based lookup function for the B+ tree is shown in \autoref{lst:bptree_lookup_coro}.
Similar to the previous data structures, the scheduler from \autoref{lst:scheduler} is used to manage multiple coroutines.
Each coroutine processes a subset of the lookup keys based on the offset and total number of coroutines.
Like the base lookup, each coroutine starts at the root node and traverses down to the leaf node,
determining the appropriate child pointer at each internal node.
Before following the child pointer,
a prefetch instruction is issued for the selected child node followed immediately by a suspension of the coroutine.
This hitns the compiler to prefetch the node into the cache while other coroutines can execute, effectively hiding the memory latency.
Once the coroutine is resumed in a round-robin fashion from the scheduler,
it continues traversing down the tree with prefetching the next node at each level and then suspending until reaching a leaf node.
The lookup in the leaf node is performed exactly as in the base lookup implementation,
with the eventual result being written into the results vector passed by reference into the coroutine function.

\begin{lstlisting}[language=C++, caption={B+ Tree lookup coroutine}, label={lst:bptree_lookup_coro}]
template<typename Key_t, typename Value_t, const size_t numCoroutines>
Task lookup(const BPTree<Key_t, Value_t> &t, const auto &lookups,
                auto &results, const size_t offset) noexcept {
    auto idx = offset; uint64_t key;

    for (; idx < lookups.size(); idx += numCoroutines) {
        key = lookups[idx];
        auto node = t.root;
        while (!node->isLeaf()) {
            size_t i = 0;
            while (i < node->nr_keys && key >= node->keys[i]) { i++; }
            PREFETCH(std::move(dynamic_cast<PtrNode<Key_t, Value_t> *>(node)->children[i]));
            co_await std::suspend_always{};
            node = dynamic_cast<PtrNode<Key_t, Value_t> *>(node)->children[i];
        }
        LeafNode<Key_t, Value_t> *leafNode =
                std::move(dynamic_cast<LeafNode<Key_t, Value_t> *>(node));
        for (size_t i = 0; i < leafNode->nr_keys; i++) {
            if (leafNode->keys[i] == key) {
                results.push_back(std::make_tuple(std::move(key), std::move(leafNode->values[i])));
                break; // Stop searching if key is found
            }
        }
    }
    co_return;
}
\end{lstlisting}


% ========= Benchmarking Setup =============
\section{Benchmarking Methodology}\label{sec:BenchmarkingSetup}

\subsection{Testing Coroutines in Different DataStructures with different compilers across Different Machines}\label{CoroutinesDifferentDataStructuresCompilersMachines}
%TODO explain which data structures are tested with coroutines, which compilers and which machines (hardware architectures)

For Hashtable one enty is of size 24 bytes (key, value each and next pointer each 8 bytes).


Number of lookups is always twice as number of insertions.

Datasizes from 2KB to at least 1GB, in many cases up to 8BG

\subsection{Used Benchmarking Frameworks}
To evaluate the coroutine implementations, two benchmarking setups are used: Google Benchmark and the perf-cpp framework developed at TU Dortmund.
Google Benchmark offers a quick way to prototype individual microbenchmarks and is therefore used early on to debug coroutine state machines, validate
correctness of lookups under different prefetch distances, and sanity-check throughput improvements on a small scale.
Its minimal configuration effort and familiar API is convenient for short, focused experiments,
especially in the beginning when iterating over scheduler changes or changes in the coroutine lookup functions themselves.
Despite its convenience, Google Benchmark has limitations in terms of flexibility for constructing complex benchmark scenarios
that involve multiple data structures, shared setup code, and detailed performance measurements using hardware counters.
Even though, fixtures are availablable in Google Benchmark, they lack the capability to share complex setup logic across multiple benchmarks
(e.g., building a hash table once and reusing it across different lookup benchmarks with varying coroutine parameters)
and using templated functions evaluated at build time to generate multiple benchmark instances with different configurations.

For the final evaluation, perf-cpp is used as the primary framework because it provides richer control over benchmark composition, data structure
construction and re-use across different benchmarks, and parameter sweeps.
The framework allows building reusable benchmark fixtures that set up hash tables or trees once and then run multiple lookup benchmarks against the same data structures.
Additionally, perf-cpp supports detailed performance measurements using hardware counters, enabling fine-grained analysis of cache misses, branch mispredictions,
and other low-level metrics relevant to evaluating coroutine performance.

This flexibility was essential to compare different coroutine designs
(e.g., varying the number of reused coroutine instances or altering prefetch strategies) under consistent data
and workload characteristics without duplicating setup code.

In conclusion, Google Benchmark serves as a pointwise verifier, where all the benchmark scenarios are mirrored but with less flexibility
in terms of choosing datasize, reusing datastructures for several different lookup functions and setting several parameters at once.
However, all reported measurements and conclusions in this thesis rely on perf-cpp runs, as that framework proved more expressive
and better suited for constructing realistic, repeatable experiments across the evaluated data structures.


% for duckdb integration duckdb's itnegrated benchmarking framework is used,
% should i mention this here if i put the duckdb integration in the data structures chapter? maybe, but probably better to have dedicated duckdb chapter


\subsection{Benchmark configuration and recorded metrics}\label{subsec:BenchmarkConfigAndRecordedMetrics}
%TODO explain which metrics are recorded and how the benchmarks are configured (e.g., number of runs, warm-up iterations, etc.)

Nr of lookups is always twice as number of insertions.


Different number of corouitnes, using with threading, using with different data sizes,
prefetch distances, ...


