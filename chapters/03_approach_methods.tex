% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

%% TODOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO

% â€¢ Describing different methods: my method was kind of vorgegeben, nur warum nicht amac und gp
% â€¢ Indicating a specific method:
% â€¢ Giving reasons why a particular method was adopted
% â€¢ Indicating sample size and characteristics
% â€¢ Indicating reasons for sample characteristics
% â€¢ Describing the process: Indicating problems or limitations

\chapter{Method to hide cache misses with coroutines}\label{chapter:MethodCoroutinesHidingCacheMisses}

This chapter describes the history of different methods that were used to implement coroutines for hiding cache misses in data structures.
It also explains why coroutines and cache prefetching were chosen as the method to hide cache misses in pointer chasing data structures
and which method is superior to others in terms of performance and implementation complexity.
% However, software prefetching does not work if the data addresses are not known in advance. To the operations in pointerchasing applications, e.g., traversing skip lists, looking up hash tables and searching trees, the addresses of next nodes are not known
% until current nodes are processed. The time between when the addresses are known and the data are needed is much less than the
% desirable prefetching distance. We call the accesses with such a pattern as immediate memory accesses. Such accesses can only benefit
% from the software prefetching by taking inter-task parallelism [21].
% Specifically, after a task issues memory prefetching, it will not continue because the prefetched data is not ready in time. Instead, the
% execution switches to other tasks, and then goes back to process the
% prefetched data after a while.


\section{Why using Coroutines combined with Software Prefetching?}
This thesis focuses on using coroutines and cache prefetching to hide cache misses in pointer chasing data structures.
The reason for this choice of using coroutines  (see \autoref{sec:CppCoroutines}) and software prefeteching ()\autoref{subsec:SoftwarePrefetching}) is,
that coroutines offer an alternative that combines the benefits of AMAC and GP approaches:
they enable interleaved execution like AMAC while preserving the synchronous programming model like GP,
but with compiler-generated context-switching code rather than manual state machine implementation.
Suspension points like co\_await or co\_yield can be simply added after prefetch operations,
and the compiler handles the the efficient state management and context switching by transforming the coroutine function into a state machine.

As Coroutines are already widely used in asynchronous programming to handle I/O-bound tasks without blocking threads successfully in C++ applications,
this thesis explores their potential in hiding cache misses in pointer-chasing data structures across machines and compilers.


% TODOOO: explain why using prefetchT0YPE_T0 vs T1 or T2


% Pointer chasing data structures, such as hash tables with chaining or linear probing, and tree-based structures like B-trees or binary search trees,
% often suffer from cache misses due to their non-contiguous memory access patterns.
% When traversing these data structures, the CPU frequently encounters cache misses, leading to significant performance degradation as it waits for data to be fetched from main memory.
% To mitigate this issue, software prefetching is employed to proactively load data into the CPU cache before it is actually needed.
% By issuing prefetch instructions ahead of time, the CPU can overlap memory access latency with computation,
% thereby reducing the time spent waiting for data to arrive.
% Coroutines provide a powerful mechanism to implement this prefetching strategy effectively.

% more on why this thesis really focusses on coorutines? and this specific use case of hiding cache misses with the new c++20 coroutines?
% comparison to amac and bulk prefetching

% With the recent standardization in C++20 [19], coroutines greatly ease the implementation of software prefetching. Coroutines [38] are functions that can suspend voluntarily and be resumed later.
% Functions that involve pointer chasing can be written as coroutines which are executed (interleaved) in batches.
% Before dereferencing a pointer in coroutine ð‘¡1, the thread issues a prefetch followed by a suspend to pause ð‘¡1 and switches to another coroutine ð‘¡2,
% overlapping data fetching in ð‘¡1 and computation in ð‘¡2. Compared to earlier approaches [5, 26], coroutines only require
% prefetch/suspend be inserted into sequential code, greatly simplifying implementation while delivering high performance, as the switching overhead can be cheaper than a last-level cache miss [21].
% However, adopting software prefetching remains challenging. First, existing approaches typically use intra-transaction batching which mandates multi-key interfaces that can break backward compatibility.
% For example, in Figure 1(b) an application1 uses multi_get to retrieve a batch of records at once in a transaction. Cache misses caused by probing k1 (k2) in a tree are hidden behind the computation part of probing k2 (k1). While intratransaction batching is a natural fit for some operators (e.g., INpredicate queries [44, 45]), it is not always directly applicable.
% Changing the application is not always feasible and may not achieve the desired improvement as depending requests need to be issued in
% The â€œapplicationâ€ may be another database system component or an end-user application that uses the record access interfaces provided by the database engine.
% separate batches, limiting interleaving opportunities. Short (or even single-record) transactions also cannot benefit much due to the lack of interleaving opportunity. It would be desirable to allow batching operations across transactions, i.e., inter-transaction batching.
% Second, prior work provided only piece-wise solutions, focusing on optimizing individual database operations (e.g., index traversal [21] and hash join [5, 44]). Despite the significant improvement (e.g., up to 3Ã— faster for tree probing [21]), it was not clear how much
% overall improvement one can expect when these techniques are applied in a full database engine that involves various components.
% Overall, these issues lead to two key questions:
% â€¢ How should a database engine adopt coroutine-based software prefetching, preferably without requiring application changes?
% â€¢ How much end-to-end benefit can software prefetching bring to a database engine under realistic workloads?


% \subsection{Different Methods using Coroutines}\label{subsec:DifferentMethodsUsingCoroutines}
\subsection{Different coroutine approaches and their trade-offs}\label{subsec:}
% Citation test~\parencite{latex}.

During the development of coroutine-based data structure lookups, the method of implementing coroutines evolved through several iterations.
The following paragraphs describe the different approaches taken, their trade-offs, and the reasoning behind the final chosen method.

The first approach used is the most straightforward one, which is creating a new coroutine instance for each lookup value/operation.
This means, each time a lookup is performed, a new coroutine is instantiated, executed until the first suspension point (after prefetching),
and then destroyed once the lookup is complete. This involves significant overhead due to the frequent creation and destruction of coroutine state machines,
which includes allocating stack space and managing the coroutine's context, including saving to the heap as explained in \autoref{sec:CppCoroutines}.
Despite being the method most easiest to understand,
having to do all this overhead implicates that this method is inferior in performance compared to other methods.

This major drawback leads to the second approach, which is to create a fixed number of coroutines at the beginning of the lookup process
and to re-use coroutines so the creation overhead is minimal and the lookup process can benefit from interleaved execution of multiple lookups.
With this second approach, the implemented coroutine function for the lookup could now leverage co\_yield to return found values back to the caller,
while the caller is responsible for resuming the coroutine until all lookups are processed.
Furthermore, the caller is responsible for distinguishing between three states:
\begin{itemize}
    \item 1) the lookup is not yet finished (the coroutine yielded without finding the value yet),
    \item 2) the lookup found the value (the coroutine yielded with a found value), and
    \item 3) the lookup did not find the value (the coroutine finished without yielding a value).
\end{itemize}

Looking at this, it is not a binary choice between found vs. not found, but a ternary choice that the caller has to handle.
To distinguish between these three states, as solution, the coroutine function could yield an optional value type,
where a value indicates that the lookup found the value, and a nullopt indicates that the lookup is not yet finished.
However, this requires eihter a fundamental change to the data structures implementation, having a nullopt value type,
or constructing a wrapper type that can hold either a valid value or a nullopt making the lookup procedure more complex.

This complexity and the need to handle three different states led to the final approach, which is to pass a pointer/reference to the result value into the coroutine function.
The coroutine function then writes the found value into the provided pointer/reference when the lookup is successful.
If the lookup is not yet finished, the coroutine simply suspends without modifying the pointer/reference.

This final approach is from this point on used for all coroutine implementations in different data structures throughout this thesis.

% Verlauf von meiner Coroutines usage:
% erst fÃ¼r jeden part eine coroutine erstellt und wiedre gelÃ¶scht -> ineffizient, weil erstellen von coroutinen viel Zeit und stack storage braucht.
% Dann mit N (e.g. =5,10,...) wiederverwendbaren Coroutinen, die erst zerstÃ¶rt werden, wenn alles durch, und imt co_yield und co_await zu suchenden Key gegeben
% und coroutine yielded (nicht mehr returned) Value zurÃ¼ck, wenn gefunden! (Problem: Unterscheidung zwischen noch nicht gefunden in HT, weil noch weiter in chaining durchtraversieren oder in linear
% weiter durchspringen muss, vs schon gefunden vs. nicht in Hashtable da (also nicht auffindbar, da Key nicht existiert)).
% Handhabarkeit schwierig, da wenn nicht gefunden, dann nullpointer zurÃ¼ck, wenn gefunden dann obviously den Value, aber was wenn "noch" nicht gefunden, da in der CHain/Linear HT
% noch weiter traversiert werden muss (aber ich das mit prefetches ebenfalls optimieren will und da dann nicht synchron langsam vorgehen will) ->
% unterschied zwischen nullpointe,r optional value/definierter Value der dafÃ¼r steht dass noch nicht gefunden UND Value (gefunden).

% Finaler Versuch, mit wiederverwendbaren Coroutinen und niht mehr co_yield aber mit pointer, wo dann bei Erfolg in Pointer zurÃ¼ckgeschrieben wird.
% -> Vorteil von wiederverwendbaren Coroutinen und einfacher handhabbar mit co_await auch wenn nicht gefunden oder "noch" nicht.


\subsection{Scheduler for multiple coroutines}\label{subsec:SchedulerForCoroutines}
As in previous section \ref{subsec:DifferentMethodsUsingCoroutines}, the final approach uses a fixed number of coroutines that are created once at the beginning of the lookup process and re-used for multiple lookups.
To manage the execution of these multiple coroutines,
a simple scheduler is implemented that resumes each coroutine in a round-robin fashion until all lookups are processed, illustrated in \autoref{lst:scheduler}.

\begin{lstlisting}[language=C++, caption={Scheduler for lookup}, label={lst:scheduler}]
template<const size_t numCoroutines>
std::vector<typename resultType>
lookupEntries(datastructure ds, lookupsVector lookups) noexcept {
    resultType result;
    std::array<Silent, numCoroutines> coroutines =
        ([&ht, &lookups, &result]<size_t... Is>(std::index_sequence<Is...>) {
            return std::array{lookup<numCoroutines>(ht, lookups, result, Is)...};
        })(std::make_index_sequence<numCoroutines>{});
    while (true) {
        size_t activeCoroutines = 0;
        for (auto &c: coroutines) {
            if (!c.h_.done()) {
                activeCoroutines++;
                c.h_.resume();
            }
        }
        if (activeCoroutines == 0) { break; }
    }
    return result;
}
\end{lstlisting}

Comparing this independently developed and implemented scheduler to the one used in CoroBase \parencite{corobase} (see \autoref{lst:corobase_scheduler}),
both schedulers follow a similar round-robin approach to resume coroutines until all tasks are completed.
However, the CoroBase scheduler is presented in a more abstract pseudocode format, while the implemented scheduler in \autoref{lst:scheduler} is written in concrete C++ code,
showing how to create and manage an array of coroutine instances using C++ templates and standard library features.
Especially, the efficient use of C++ template metaprogramming to create an array of coroutines at compile time is a notable difference
and the handling of checking if all coroutines done.

\begin{lstlisting}[language=Python, caption={CoroBase Scheduler for lookup}, label={lst:corobase_scheduler}]
for i = 0 to batch_size - 1:
    coroutine[i] = foo(...);
while any(coroutine_promises, x: not x.done()):
    for i = 0 to batch_size - 1:
        if not coroutine[i].done():
            coroutine[i].resume()
\end{lstlisting}


% ========= Using Coroutines in data structures =============
\section{Using Coroutines in different data structures}\label{sec:UsingCoroutinesInDifferentDataStructures}

This section describes the integration of coroutines into different pointer-chasing data structures by using aforementioned scheduler.
It explains how coroutines are integrated into chaining hash tables, linear probing hash tables, and B+ trees.
For each data structure, the implementation of the coroutine-based lookup function is presented,
pointing out details and key aspects of the implementation.

% ----------- Hashtable -----------
\subsection{Coroutines in chaining Hashtable}\label{sec:CoroutinesInHashtable}

\begin{lstlisting}[language=C, frame=single, caption={coroutine lookup in chaining hashtable}, label={lst:coroutine_ht_lookup}]
template<const size_t numCoroutines>
    Task lookup(const ht_primary_key::Hashtable &ht, const auto &lookups, auto &results,
                  const size_t offset) noexcept {
        auto idx = offset; size_t key; uint64_t hashIdx;

        for (; idx < lookups.size(); idx += numCoroutines) {
            key = lookups[idx];
            hashIdx = hashKey(key) & ht.mask;
            PREFETCH(&(ht.ht[hashIdx]));    // Initial prefetch of the first value
            co_await std::suspend_always{}; // Suspend the coroutine until resumed

            auto result = ht.ht[hashIdx]; // Read the previously cached value

            while (!((result == nullptr) || (result->key == key))) {
                PREFETCH(result->next); // Prefetch the next entry in the linked list
                co_await std::suspend_always{};
                result = result->next;
            }
            if (result != nullptr) results.push_back(result);
        }
        co_return;
    }
\end{lstlisting}


% --------- Linear Probing Hashtable ------------
\subsection{Coroutines in linear probing hashtable}\label{sec:CoroutinesInLinearProbingHashtable}


\begin{lstlisting}[language=C, frame=single, caption={linear probing hashtable coroutine lookup}, label={lst:linear_probing_ht_lookup_coro}]
template<const size_t numCoroutines>
    Task lookup_ht_entry_t(const linear_probing_ht_duckdb::LinearProbingHashTable<uint64_t, uint64_t> &ht,
                             const auto &lookups, auto &results, const size_t offset) noexcept {
        auto idx = offset;
        size_t key;
        uint64_t hashIdx;
        uint64_t hash;
        linear_probing_ht_duckdb::LinearProbingHashTable<uint64_t, uint64_t>::EntryData *data = nullptr;

        for (; idx < lookups.size(); idx += numCoroutines) {
            key = lookups[idx];
            hash = ht.ComputeHash(key);
            uint64_t slot = hash & ht.bitmask;
            uint64_t salt = ht.ExtractSalt(hash);

            // Find SLOT
            linear_probing_ht_duckdb::ht_entry_t entry;
            while (true) {
                PREFETCH(&(ht.entries[slot]));  // Initial prefetch of the first value
                co_await std::suspend_always{}; // Suspend the coroutine until resumed
                entry = ht.entries[slot];       // Read the previously cached value
                if (!entry.IsOccupied()) {
                    // Empty slot found
                    break;
                }
                if (ht.use_salts) {
                    // Check salt first (bloom filter)
                    if (entry.GetSalt() == salt) {
                        // Salt matches, need to check actual key
                        data = reinterpret_cast<
                                linear_probing_ht_duckdb::LinearProbingHashTable<uint64_t, uint64_t>::EntryData *>(
                                entry.GetPointer());
                        if (data && ht.key_equal(data->key, key)) {
                            entry = ht.entries[slot]; // Found matching key
                            break;
                        }
                    }
                } else {
                    // No salt optimization, check key directly
                    data = reinterpret_cast<
                            linear_probing_ht_duckdb::LinearProbingHashTable<uint64_t, uint64_t>::EntryData *>(
                            entry.GetPointer());
                    if (data && ht.key_equal(data->key, key)) {
                        entry = ht.entries[slot]; // Found matching key
                        break;
                    }
                }

                // Continue linear probing
                data = nullptr;
                linear_probing_ht_duckdb::IncrementAndWrap(slot, ht.bitmask);
            }

            if(data == nullptr) {
                continue; // Not found
            }
            // Found an entry, retrieve all values in the chain
            linear_probing_ht_duckdb::LinearProbingHashTable<uint64_t, uint64_t>::EntryData *current = data;
            while (current) {
                results.push_back(current);
                PREFETCH(current->next);
                co_await std::suspend_always();
                current = current->next;
            }
        }

        co_return;
    }
\end{lstlisting}




% --------- B+ Tree ------------
\subsection{Coroutines in B+ Tree}\label{sec:CoroutinesInBPlusTree}

\begin{lstlisting}[language=C++, caption={B+ Tree lookup coroutine}, label={lst:bptree_lookup_coro}]
template<typename Key_t, typename Value_t, const size_t numCoroutines>
    Task lookup(const bptree::BPTree<Key_t, Value_t> &t, const auto &lookups, auto &results,
                  const size_t offset) noexcept {
        auto idx = offset;
        uint64_t key;

        for (; idx < lookups.size(); idx += numCoroutines) {
            key = lookups[idx];

            auto node = t.root;
            while (!node->isLeaf()) {
                size_t i = 0;
                while (i < node->nr_keys && key >= node->keys[i]) {
                    i++;
                }
                PREFETCH(std::move(dynamic_cast<bptree::PtrNode<Key_t, Value_t> *>(node)->children[i]));
                co_await std::suspend_always{};
                node = dynamic_cast<bptree::PtrNode<Key_t, Value_t> *>(node)->children[i];
            }
            bptree::LeafNode<Key_t, Value_t> *leafNode =
                    std::move(dynamic_cast<bptree::LeafNode<Key_t, Value_t> *>(node));

            // here I could do better search algorithm (binary search) but for now I just do linear search
            for (size_t i = 0; i < leafNode->nr_keys; i++) {
                if (leafNode->keys[i] == key) {
                    results.push_back(std::make_tuple(std::move(key), std::move(leafNode->values[i])));
                    break; // Stop searching if key is found
                }
            }
        }

        co_return;
    }
\end{lstlisting}



% ========= Benchmarking Setup =============
\section{Benchmarking Setup}\label{sec:BenchmarkingSetup}

\subsection{Testing Coroutines in Different DataStructures with different compilers across Different Machines}\label{CoroutinesDifferentDataStructuresCompilersMachines}
%TODO explain which data structures are tested with coroutines, which compilers and which machines (hardware architectures)



\subsection{Used Benchmarking Frameworks}
To evaluate the coroutine implementations, two benchmarking setups are used: Google Benchmark and the perf-cpp framework developed at TU Dortmund.
Google Benchmark offers a quick way to prototype individual microbenchmarks and is therefore used early on to debug coroutine state machines, validate
correctness of lookups under different prefetch distances, and sanity-check throughput improvements on a small scale.
Its minimal configuration effort and familiar API is convenient for short, focused experiments,
especially in the beginning when iterating over scheduler changes or changes in the coroutine lookup functions themselves.
Despite its convenience, Google Benchmark has limitations in terms of flexibility for constructing complex benchmark scenarios
that involve multiple data structures, shared setup code, and detailed performance measurements using hardware counters.
Even though, fixtures are availablable in Google Benchmark, they lack the capability to share complex setup logic across multiple benchmarks
(e.g., building a hash table once and reusing it across different lookup benchmarks with varying coroutine parameters)
and using templated functions evaluated at build time to generate multiple benchmark instances with different configurations.

For the final evaluation, perf-cpp is used as the primary framework because it provides richer control over benchmark composition, data structure
construction and re-use across different benchmarks, and parameter sweeps.
The framework allows building reusable benchmark fixtures that set up hash tables or trees once and then run multiple lookup benchmarks against the same data structures.
Additionally, perf-cpp supports detailed performance measurements using hardware counters, enabling fine-grained analysis of cache misses, branch mispredictions,
and other low-level metrics relevant to evaluating coroutine performance.

This flexibility was essential to compare different coroutine designs
(e.g., varying the number of reused coroutine instances or altering prefetch strategies) under consistent data
and workload characteristics without duplicating setup code.

In conclusion, Google Benchmark serves as a pointwise verifier, where all the benchmark scenarios are mirrored but with less flexibility
in terms of choosing datasize, reusing datastructures for several different lookup functions and setting several parameters at once.
However, all reported measurements and conclusions in this thesis rely on perf-cpp runs, as that framework proved more expressive
and better suited for constructing realistic, repeatable experiments across the evaluated data structures.


% for duckdb integration duckdb's itnegrated benchmarking framework is used,
% should i mention this here if i put the duckdb integration in the data structures chapter? maybe, but probably better to have dedicated duckdb chapter


\subsection{Benchmark configuration and recorded metrics}\label{subsec:BenchmarkConfigAndRecordedMetrics}
%TODO explain which metrics are recorded and how the benchmarks are configured (e.g., number of runs, warm-up iterations, etc.)

Different number of corouitnes, using with threading, using with different data sizes,
prefetch distances, ...


