% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

%% TODOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO

% • Describing different methods: my method was kind of vorgegeben, nur warum nicht amac und gp
% • Indicating a specific method:
% • Giving reasons why a particular method was adopted
% • Indicating sample size and characteristics
% • Indicating reasons for sample characteristics
% • Describing the process: Indicating problems or limitations

\chapter{Methodology to hide cache misses with coroutines}\label{chapter:MethodCoroutinesHidingCacheMisses}

This chapter describes the history of different methods that were used to implement coroutines for hiding cache misses in data structures.
It also explains why coroutines and cache prefetching were chosen as the method to hide cache misses in pointer chasing data structures
and which method is superior to others in terms of performance and implementation complexity.
% However, software prefetching does not work if the data addresses are not known in advance. To the operations in pointerchasing applications, e.g., traversing skip lists, looking up hash tables and searching trees, the addresses of next nodes are not known
% until current nodes are processed. The time between when the addresses are known and the data are needed is much less than the
% desirable prefetching distance. We call the accesses with such a pattern as immediate memory accesses. Such accesses can only benefit
% from the software prefetching by taking inter-task parallelism [21].
% Specifically, after a task issues memory prefetching, it will not continue because the prefetched data is not ready in time. Instead, the
% execution switches to other tasks, and then goes back to process the
% prefetched data after a while.


\section{Why using Coroutines combined with Software Prefetching?}
This thesis focuses on using coroutines and cache prefetching to hide cache misses in pointer chasing data structures.
The reason for this choice of using coroutines  (see \autoref{sec:CppCoroutines}) and software prefeteching ()\autoref{subsec:SoftwarePrefetching}) is,
that coroutines offer an alternative that combines the benefits of AMAC and GP approaches:
they enable interleaved execution like AMAC while preserving the synchronous programming model like GP,
but with compiler-generated context-switching code rather than manual state machine implementation.
Suspension points like co\_await or co\_yield can be simply added after prefetch operations,
and the compiler handles the the efficient state management and context switching by transforming the coroutine function into a state machine.
This significantly reduces the implementation complexity compared to AMAC, and also to GP, as the coroutine syntax allows writing code in a straightforward sequential manner.

As Coroutines are already widely used in asynchronous programming to handle I/O-bound tasks without blocking threads successfully in C++ applications,
this thesis explores their potential in hiding cache misses in pointer-chasing data structures across machines and compilers.

For the pretch operation itself, the x86 intrinsic non-temporal prefetchnta operation is used as shown in \autoref{lst:prefetch_macro}.
Both, Intel and AMD (for later generations, e.g. Zen) of their CPUs do not specify a particular cache taget, however, they both state
that non-temporally prefetches are prioritized for quicker eviction from the cache hierarchy.
This reduces cache pollution for data that is not expected to be reused soon, which is exactly the case in pointer-chasing data structures
where access patterns are irregular and data is not reused frequently.
\begin{lstlisting}[language=C, frame=single, caption={Prefetch macro}, label={lst:prefetch_macro}]
#define PREFETCH(addr) _mm_prefetch(addr, _MM_HINT_NTA)
\end{lstlisting}

To optimally utilize coroutines with software prefetching, multiple coroutines are needed to interleave their execution as independent instructions have to be scheduled after the prefetch to hide cache misses.
When one coroutine issues a prefetch and suspends itself, other coroutines can run and issue their own prefetches,
thus keeping the CPU busy while waiting for data to be fetched into the cache.
However, the exact number of coroutines needed to effectively hide cache misses depends on various factors,
including the hardware components like Cache sizes, memory latency, line fill buffer, and the specific data structure and the size of the data entries being accessed.


\subsection{Different coroutine approaches and their trade-offs}\label{subsec:}

During the development of coroutine-based data structure lookups, the method of implementing coroutines evolved through several iterations.
The following paragraphs describe the different approaches taken, their trade-offs, and the reasoning behind the final chosen method.

The first approach used is the most straightforward one, which is creating a new coroutine instance for each lookup value/operation.
This means, each time a lookup is performed, a new coroutine is instantiated, executed until the first suspension point (after prefetching),
and then destroyed once the lookup is complete. This involves significant overhead due to the frequent creation and destruction of coroutine state machines,
which includes allocating stack space and managing the coroutine's context, including saving to the heap as explained in \autoref{sec:CppCoroutines}.
Despite being the method most easiest to understand,
having to do all this overhead implicates that this method is inferior in performance compared to other methods.

This major drawback leads to the second approach, which is to create a fixed number of coroutines at the beginning of the lookup process
and to re-use coroutines so the creation overhead is minimal and the lookup process can benefit from interleaved execution of multiple lookups.
With this second approach, the implemented coroutine function for the lookup could now leverage co\_yield to return found values back to the caller,
while the caller is responsible for resuming the coroutine until all lookups are processed.
Furthermore, the caller is responsible for distinguishing between three states:
\begin{itemize}
    \item 1) the lookup is not yet finished (the coroutine yielded without finding the value yet),
    \item 2) the lookup found the value (the coroutine yielded with a found value), and
    \item 3) the lookup did not find the value (the coroutine finished without yielding a value).
\end{itemize}

Looking at this, it is not a binary choice between found vs. not found, but a ternary choice that the caller has to handle.
To distinguish between these three states, as solution, the coroutine function could yield an optional value type,
where a value indicates that the lookup found the value, and a nullopt indicates that the lookup is not yet finished.
However, this requires eihter a fundamental change to the data structures implementation, having a nullopt value type,
or constructing a wrapper type that can hold either a valid value or a nullopt making the lookup procedure more complex.

This complexity and the need to handle three different states led to the final approach, which is to pass a pointer/reference to the result value into the coroutine function.
The coroutine function then writes the found value into the provided pointer/reference when the lookup is successful.
If the lookup is not yet finished, the coroutine simply suspends without modifying the pointer/reference.

This final approach is from this point on used for all coroutine implementations in different data structures throughout this thesis.

\subsection{Scheduler for multiple coroutines}\label{subsec:SchedulerForCoroutines}
As in previous section \ref{subsec:DifferentMethodsUsingCoroutines}, the final approach uses a fixed number of coroutines that are created once at the beginning of the lookup process and re-used for multiple lookups.
To manage the execution of these multiple coroutines,
a simple scheduler is implemented that resumes each coroutine in a round-robin fashion until all lookups are processed, illustrated in \autoref{lst:scheduler}.

\begin{lstlisting}[language=C++, caption={Scheduler for lookup}, label={lst:scheduler}, numbers=left]
template<const size_t numCoroutines>
std::vector<typename resultType>
lookupEntries(datastructure ds, lookupsVector lookups) noexcept {
    resultType result;
    std::array<Silent, numCoroutines> coroutines =
        ([&ht, &lookups, &result]<size_t... Is>(std::index_sequence<Is...>) {
            return std::array{lookup<numCoroutines>(ht, lookups, result, Is)...};
        })(std::make_index_sequence<numCoroutines>{});
    while (true) {
        size_t activeCoroutines = 0;
        for (auto &c: coroutines) {
            if (!c.h_.done()) {
                activeCoroutines++;
                c.h_.resume();
            }
        }
        if (activeCoroutines == 0) { break; }
    }
    return result;
}
\end{lstlisting}

Comparing this independently developed and implemented scheduler to the one used in CoroBase \parencite{corobase} (see \autoref{lst:corobase_scheduler}),
both schedulers follow a similar round-robin approach to resume coroutines until all tasks are completed.
However, the CoroBase scheduler is presented in a more abstract pseudocode format, while the implemented scheduler in \autoref{lst:scheduler} is written in concrete C++ code,
showing how to create and manage an array of coroutine instances using C++ templates and standard library features.
Especially, using C++ template metaprogramming to create an array of coroutines at compile time
and the handling of checking if all coroutines are finished are to be highlighted.
By counting active coroutines in each iteration, and breaking the while loop only when none are left,
the implemented scheduler reduces unnecessary checks and improves branch prediction as much as possible as only
in the one last case where all coroutines are done, the loop breaks and the branch prediction is potentially wrong.

\begin{lstlisting}[language=Python, caption={CoroBase Scheduler for lookup}, label={lst:corobase_scheduler}]
for i = 0 to batch_size - 1:
    coroutine[i] = foo(...);
while any(coroutine_promises, x: not x.done()):
    for i = 0 to batch_size - 1:
        if not coroutine[i].done():
            coroutine[i].resume()
\end{lstlisting}


% ========= Using Coroutines in data structures =============
\section{Using Coroutines in different data structures}\label{sec:UsingCoroutinesInDifferentDataStructures}

This section describes the integration of coroutines into different pointer-chasing data structures by using aforementioned scheduler.
It explains how coroutines are integrated into chaining hash tables, linear probing hash tables, and B+ trees.
For each data structure, the implementation of the coroutine-based lookup function is presented,
pointing out details and key aspects of the implementation.

% ----------- Hashtable -----------
\subsection{Coroutines in chaining Hashtable}\label{sec:CoroutinesInHashtable}
The first datastructure where coroutines are integrated is a chaining hashtable.
The chaining hashtable consists of an array of buckets,
where each bucket contains a linked list of entries that hash to the same index.
When performing a lookup, the hash index is computed from the key, and the corresponding bucket is accessed.
The linked list in the bucket is then traversed to find the entry with the matching key.
The size of the hashtable is chosen to be a power of two to allow efficient computation of the hash index using a bitwise AND operation with a mask
and is the next power of two of the handed-over size during construction.
So for all hashtables constructed the load factor is 100\% to reflect a bad hastable scenario with many collisions.
The hash function used is the MurmurHash3 algorithm.% \parencite{murmurhash3}.

The coroutine-based lookup function for the chaining hashtable is shown in \autoref{lst:coroutine_ht_lookup}.
For schedulign multiple coroutines, the scheduler from \autoref{lst:scheduler} is used.
Each coroutine processes a fixed-size subset of the lookup keys, determined by the offset parameter and the total number of coroutines,
meaning that each coroutine from start has its own index to start processing keys from and does not consider if other coroutines
are already finished with their subset of keys that they continue to process other keys from other coroutines.
The Coroutines are eagerly started and for each key, the hash index is computed, and a prefetch instruction is issued for the
corresponding bucket to bring the first entry into the cache. Directly after the prefetch, the coroutine suspends itself using co\_await.
When the coroutine is resumed, it reads the prefetched entry and eventually traverses the linked list in the bucket to find the matching key
until either the key is found or the end of the list is reached.
If a matching key for the to searching key is found, the corresponding entry is added to the results vector,
a pointer/reference passed into the coroutine function from the calling scheduler.
The prefetches are always directly followed by a suspension of the coroutine to allow other coroutines to run while the data is being fetched into the cache.
If one coroutine is finished it returns with co\_return and the scheduler will not resume it anymore.

\begin{lstlisting}[language=C, frame=single, caption={coroutine lookup in chaining hashtable}, label={lst:coroutine_ht_lookup}]
template<const size_t numCoroutines>
Task lookup(const ht_primary_key::Hashtable &ht, const auto &lookups, auto &results,
                const size_t offset) noexcept {
    auto idx = offset; size_t key; uint64_t hashIdx;

    for (; idx < lookups.size(); idx += numCoroutines) {
        key = lookups[idx];
        hashIdx = hashKey(key) & ht.mask;
        PREFETCH(&(ht.ht[hashIdx]));    // Initial prefetch of 1st value
        co_await std::suspend_always{}; // Suspend coroutine

        auto result = ht.ht[hashIdx]; // Read previously cached value
        while (!((result == nullptr) || (result->key == key))) {
            PREFETCH(result->next); // Prefetch next entry in linked list
            co_await std::suspend_always{};
            result = result->next;
        }
        if (result != nullptr) results.push_back(result);
    }
    co_return;
}
\end{lstlisting}

Note, that the base lookup function as well as a single coroutine return a pointer to the found entry in the hashtable.
Thus, the result vector passed into the coroutine function and the overall result of the base looup are a vector of pointers to hashtable entries.
% These still need to be dereferenced to access the actual key-value pairs stored in the hashtable,
% but for subsequent applications this will be efficient as the hardware prefetcher can

% --------- Linear Probing Hashtable ------------
\subsection{Coroutines in linear probing hashtable}\label{sec:CoroutinesInLinearProbingHashtable}

Linear hashtable uses open addressing with linear probing to resolve collisions.
When a collision occurs (i.e., when two keys hash to the same index), the hashtable probes the next available slot in a linear manner
until an empty slot is found or the desired key is located.
For the same key, chaining is used to store multiple entries in a linked list at the same slot.
This particular implementation of a linear probing hashtable grows dynamically by doubling in size when the load factor exceeds 70\%.

The coroutine-based lookup function for the linear probing hashtable is shown in \autoref{lst:coroutine_ht_lookup}.
For schedulign multiple coroutines, the scheduler from \autoref{lst:scheduler} is used.
Each coroutine processes a fixed-size subset of the lookup keys, determined by the offset parameter and the total number of coroutines,
meaning that each coroutine from start has its own index to start processing keys from and does not consider if other coroutines
are already finished with their subset of keys that they continue to process other keys from other coroutines.
The Coroutines are eagerly started and for each key, the hash index is computed, and a prefetch instruction is issued for the
corresponding bucket to bring the first entry into the cache. Directly after the prefetch, the coroutine suspends itself using co\_await.
When the coroutine is resumed, it reads the prefetched entry and if the slot is occupied, it checks if the key matches the searching key.
If it does not match, the coroutine continues linear probing by incrementing the slot index and prefetching the next slot, again by prefetching and suspending itself.
This continues until either a matching key is found or an empty slot is encountered, in which case the lookup concludes without a result.
However, if a matching key for the to searching key is found, the corresponding entry is added to the results vector,
and the chain of entries at that slot is traversed to retrieve all entries with the same key,
by prefetching each next entry in the chain and suspending the coroutine after each prefetch.
The found entries are added to the results vector, a pointer/reference passed into the coroutine function from the calling scheduler.

\begin{lstlisting}[language=C, frame=single, caption={linear probing hashtable coroutine lookup}, label={lst:linear_probing_ht_lookup_coro}]
template<const size_t numCoroutines>
Task lookup_ht_entry_t(const LinearProbingHashTable<uint64_t, uint64_t> &ht,
                            const auto &lookups, auto &results, const size_t offset) noexcept {
    auto idx = offset; size_t key; uint64_t hashIdx; uint64_t hash;
    LinearProbingHashTable<uint64_t, uint64_t>::EntryData *data = nullptr;

    for (; idx < lookups.size(); idx += numCoroutines) {
        key = lookups[idx];
        hash = ht.ComputeHash(key);
        uint64_t slot = hash & ht.bitmask;
        uint64_t salt = ht.ExtractSalt(hash);

        // Find SLOT
        ht_entry_t entry;
        while (true) {
            PREFETCH(&(ht.entries[slot]));  // Initial prefetch of the first value
            co_await std::suspend_always{}; // Suspend the coroutine until resumed
            entry = ht.entries[slot];       // Read the previously cached value
            if (!entry.IsOccupied()) { break; } // Empty slot found
            if (ht.use_salts) {
                if (entry.GetSalt() == salt) {
                    data = reinterpret_cast<
                            LinearProbingHashTable<uint64_t, uint64_t>::EntryData *>(
                            entry.GetPointer());
                    if (data && ht.key_equal(data->key, key)) {
                        entry = ht.entries[slot]; break; } // Found matching key
                }
            } else {
                data = reinterpret_cast<LinearProbingHashTable<
                        uint64_t, uint64_t>::EntryData *>(entry.GetPointer());
                if (data && ht.key_equal(data->key, key)) {
                    entry = ht.entries[slot]; break; }  // Found matching key
            }
            // Continue linear probing
            data = nullptr;
            IncrementAndWrap(slot, ht.bitmask);
        }

        if(data == nullptr) { continue; } // Not found
        // Found an entry, retrieve all values in the chain
        LinearProbingHashTable<uint64_t, uint64_t>::EntryData *current = data;
        while (current) {
            results.push_back(current);
            PREFETCH(current->next);
            co_await std::suspend_always();
            current = current->next;
        }
    }
    co_return;
}
\end{lstlisting}

Note, that the normal lookup as well as the coroutine lookup function return a pointer to the found entry in the hashtable,
ending up in a vector of pointers to hashtable entries as the overall result.

% --------- B+ Tree ------------
\subsection{Coroutines in B+ Tree}\label{sec:CoroutinesInBPlusTree}
The B+ tree base lookup implementation traverses the tree from root to leaf to locate keys in the data structure.
The tree is partitioned into internal nodes, which contain routing information, and leaf nodes,
which store the actual key-value pairs. Each node contains a pointer to its parent node, an array of keys, the current number of keys,
and either an array of child pointers (for internal nodes) or an array of values (for leaf nodes).
For a given lookup key, the algorithm starts at the root node and iteratively descends through the tree.
At each internal node, the keys are searched to determine which child subtree should be examined next.
The current implementation uses a sequential linear scan where the comparison \texttt{key >= node->keys[i]} identifies the correct position
in the sorted key array of the internal node and thus the appropriate child pointer to follow.
This binary tree property ensures that all keys smaller than the branching key are in the left subtree,
while keys greater than or equal to it are in the right subtree.
For larger nodes, the additionally implemented binary search could reduce the number of comparisons from $O(n)$ to $O(\log n)$,
though the linear scan is often sufficient for typical B+ tree node sizes being by default $128$ due to cache efficiency.
When reaching a leaf node, the algorithm performs a final linear search of the keys to find a match.
If a matching key is found, the corresponding value is returned else the lookup concludes without a result.

The coroutine-based lookup function for the B+ tree is shown in \autoref{lst:bptree_lookup_coro}.
Similar to the previous data structures, the scheduler from \autoref{lst:scheduler} is used to manage multiple coroutines.
Each coroutine processes a subset of the lookup keys based on the offset and total number of coroutines.
Like the base lookup, each coroutine starts at the root node and traverses down to the leaf node,
determining the appropriate child pointer at each internal node.
Before following the child pointer,
a prefetch instruction is issued for the selected child node followed immediately by a suspension of the coroutine.
This hitns the compiler to prefetch the node into the cache while other coroutines can execute, effectively hiding the memory latency.
Once the coroutine is resumed in a round-robin fashion from the scheduler,
it continues traversing down the tree with prefetching the next node at each level and then suspending until reaching a leaf node.
The lookup in the leaf node is performed exactly as in the base lookup implementation,
with the eventual result being written into the results vector passed by reference into the coroutine function.

\begin{lstlisting}[language=C++, caption={B+ Tree lookup coroutine}, label={lst:bptree_lookup_coro}]
template<typename Key_t, typename Value_t, const size_t numCoroutines>
Task lookup(const BPTree<Key_t, Value_t> &t, const auto &lookups,
                auto &results, const size_t offset) noexcept {
    auto idx = offset; uint64_t key;

    for (; idx < lookups.size(); idx += numCoroutines) {
        key = lookups[idx];
        auto node = t.root;
        while (!node->isLeaf()) {
            size_t i = 0;
            while (i < node->nr_keys && key >= node->keys[i]) { i++; }
            PREFETCH(std::move(dynamic_cast<PtrNode<Key_t, Value_t> *>(node)->children[i]));
            co_await std::suspend_always{};
            node = dynamic_cast<PtrNode<Key_t, Value_t> *>(node)->children[i];
        }
        LeafNode<Key_t, Value_t> *leafNode =
                std::move(dynamic_cast<LeafNode<Key_t, Value_t> *>(node));
        for (size_t i = 0; i < leafNode->nr_keys; i++) {
            if (leafNode->keys[i] == key) {
                results.push_back(std::make_tuple(std::move(key), std::move(leafNode->values[i])));
                break; // Stop searching if key is found
            }
        }
    }
    co_return;
}
\end{lstlisting}

Note, that in this B+ tree implementation, values and keys are stored in separate arrays only returning the value for a given key.
This makes it necessary for the results vector to store both the key and value as a tuple,
so that the caller can associate the found value with its corresponding key as coroutines potentially return results out of order if one coroutine is faster than another.

% ========= Benchmarking Setup =============
\section{Benchmarking Methodology}\label{sec:BenchmarkingSetup}

\subsection{Benchmark Configuration and Setup}\label{CoroutinesDifferentDataStructuresCompilersMachines}
To evaluate the performance of coroutine-based data structure lookups across different data structures, machines, and compilers,
a comprehensive benchmarking setup is established.
Each data structure (chaining hash table, linear probing hash table, B+ tree) is implemented with coroutine-based lookups as described in \autoref{sec:UsingCoroutinesInDifferentDataStructures}
and a base looup implementation without coroutines and without prefetching for comparison.

To have a fair comparison across the data structures, the amount of data inserted is measured in data size rather than number of entries, as different data structures have different entry sizes.
The entries inserted into the data structures as well as the lookups are independently randomly generated keys and values, where it is to emphasize that
the number of lookups is always twice as number of insertions to simulate a realistic workload with more lookups than insertions.
The amount of data inserted into the data structures varies from 2KB to at least 1GB, in many cases up to 8GB, to evaluate performance across different data sizes.
This range of data sizes allows observing how coroutine performance scales with increasing data sizes, especially as data sizes exceed various cache levels (L1, L2, L3).

\paragraph{Overview of different data structures}
To verify the effectiveness of coroutines in hiding cache misses across different pointer-chasing data structures,
three widely used data structures are selected for evaluation: chaining hash tables, linear probing hash tables, and B+ trees.

For the hash table one enty is of size 24 bytes (key, value each and next pointer each 8 bytes).

\paragraph{Overview of different machines}
Furthermore to evaluate the performance of coroutine implementations across different hardware architectures,
multiple machines with varying CPU architectures are used.
This includes following four machines:
\begin{itemize}
    \item Skylake:
    \item Skylake X:
    \item AMD Threadripper:
    \item Xeon (Sapphire Rapids):
\end{itemize}

More details about the machines and their specifications are provided in TODO. % \autoref{tab:machines}. im Anhang?

\paragraph{Overview of different compilers}
Additionally to benchmarking on different machines, multiple compilers are used to evaluate the performance of coroutine implementations.
This includes following compilers:
\begin{itemize}
    \item GCC 14
    \item Clang 18.0
    \item Clang 20.0
\end{itemize}
Goal is to see how different compilers optimize the coroutine state machines and the generated prefetch instructions,
as well as how well the different compilers support C++20 coroutines in general.
% The reason for choosing these specific compiler versions is that they are the latest stable releases at the time of writing this thesis,
% and they all have mature support for C++20 features, including coroutines.

% CLang 18 and 20 used, weil Heap elision with coroutines stuff


\paragraph{Different number of Threads}
To further explore the performance of coroutine-based lookups under varying levels of concurrency,
the benchmarks are executed with different numbers of threads, ranging from single-threaded execution to multiple threads, e.g. 2, 4, 8, 16 threads.



\subsection{Benchmark configuration and recorded metrics}\label{subsec:BenchmarkConfigAndRecordedMetrics}
%TODO explain which metrics are recorded and how the benchmarks are configured (e.g., number of runs, warm-up iterations, etc.)
% maybe consolidate in one subsection with the previous one




\subsection{Used Benchmarking Frameworks}
To evaluate the coroutine implementations, two benchmarking setups are used: Google Benchmark and the perf-cpp framework developed at TU Dortmund.
Google Benchmark offers a quick way to prototype individual microbenchmarks and is therefore used early on to debug coroutine state machines, validate
correctness of lookups under different prefetch distances, and sanity-check throughput improvements on a small scale.
Its minimal configuration effort and familiar API is convenient for short, focused experiments,
especially in the beginning when iterating over scheduler changes or changes in the coroutine lookup functions themselves.
Despite its convenience, Google Benchmark has limitations in terms of flexibility for constructing complex benchmark scenarios
that involve multiple data structures, shared setup code, and detailed performance measurements using hardware counters.
Even though, fixtures are availablable in Google Benchmark, they lack the capability to share complex setup logic across multiple benchmarks
(e.g., building a hash table once and reusing it across different lookup benchmarks with varying coroutine parameters)
and using templated functions evaluated at build time to generate multiple benchmark instances with different configurations.

For the final evaluation, perf-cpp is used as the primary framework because it provides richer control over benchmark composition, data structure
construction and re-use across different benchmarks, and parameter sweeps.
The framework allows building reusable benchmark fixtures that set up hash tables or trees once and then run multiple lookup benchmarks against the same data structures.
Additionally, perf-cpp supports detailed performance measurements using hardware counters, enabling fine-grained analysis of cache misses, branch mispredictions,
and other low-level metrics relevant to evaluating coroutine performance.
In conclusion, all reported measurements and conclusions in this thesis rely on perf-cpp runs,
and Google Benchmark is only used for prototyping and sanity-checking individual scenarios.
