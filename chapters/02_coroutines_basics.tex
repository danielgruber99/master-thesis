% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.



% • Describing different methods
% • Indicating a specific method
% • Giving reasons why a particular method was adopted
% • Indicating sample size and characteristics
% • Indicating reasons for sample characteristics
% • Describing the process: Indicating problems or limitations


%%% ====== COROUTINES BASICS =======================
\chapter{Coroutines and Cache Misses Fundamentals}\label{chapter:CacheCoroDSFundamentals}

%%% ====== COROUTINES =======================
\section{C++ Coroutines}\label{sec:CppCoroutines}
% \section{C++ Coroutines Coroutine CPP Reference standardisation history}\label{section:Coroutines}
% Citation test~\parencite{latex}.

%%% ------- Intro to COROUTINES ------------------------
\subsection{Introduction to Coroutines}\label{subsec:IntroductionToCoroutines}
C++ Coroutines are available in the std namespace from the C++20 standard on, and from the C++23 standard there is a generator implementation based on the C++ 20 coroutine concept.
% https://en.cppreference.com/w/cpp/language/coroutines

A coroutine is a generalization of a function, being able additionally to normal functions to suspended execution and resumed it later on.
Any function is a coroutine if its definition contains at least one of these three keywords:
\begin{itemize}
  \item co\_await - to suspend execution
  \item co\_yield - to suspend execution and returning a value
  \item co\_return - to complete exeuction and returning a value
\end{itemize}
C++ Coroutines are stackless, meaning by suspension and consecutively returning to the caller, the data of the coroutine is stored separately from the stack, namely on the heap.
This allows sequential code to be executed asynchronously, without blocking the thread of execution and supports algorithms to be lazily computed, e.g. generators.
However, there are some restrictions to coroutines: They cannot use variadic arguments, plain return statements or placeholder return types likes auto or Concept.
Also consteval, conexpr and the main function as well as constructors and destructors cannot be coroutines.

To illustrate the difference between coroutines and functions, this paragraph provides a background and general information of function calls and return:
A normal function has a single entry point - the Call operation - and a single exit point - the Return operation.
The Call operation creates an activation frame, suspends execution of the caller and transfers execution to the callee, where the caller is the invocating function and the callee is the invocated function.
The Return operation returns the value in the return statement to the caller, destroys the activation frame and then resumes execution of the caller.
These operations include calling conventions splitting the responsibilites of the caller and callee regarding saving register values to their activation frames.
The activation frame is also commonly called stack frame, as the functions state (parameters, local variables) are stored on the stack.
Normal Functions have strictly nested lifetimes, meaning they run synchronously from start to finish, allowing the stack to be a highly efficient memory allocation data-structure for allocation and freeing frames.
The pointer pointing at the top of the stack is the **rsp** register on X86-64 CPU Architectures.

Coroutines have, additionally to the call and return operation, three extra operations, namely suspend, resume and destroy.
As coroutines can suspend execution without destroying the activation frame, as it may be resumed later, the activation frames are not strictly nested anymore.
This requires that after the creation of the coroutine on the stack, the state of the coroutine is saved to the heap,
like illustrated in \autoref{fig:CallingACoroutineHeap} where a normal function f() calls a coroutine function c().
% https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/n4680.pdf

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\paperwidth]{figures/coroutines_drawing2_coro_call_heap.png}
  \caption{Calling a Coroutine - Heap Allocation}
  % \quelle{\cite{ADaSurvey}}
  \label{fig:CallingACoroutineHeap}
\end{figure}

If the coroutine calls another normal function g(), g() 's activation frame is created on the stack and the coroutine stack frame points to the heap allocated frame,
 as illustrated in \autoref{fig:CoroutineCallsNormalFunction}. When g() returns, it destroys its activation frame and restores c()'s activation frame.
\begin{figure}[h]
  \begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/coroutines_drawing3_coro_calls_other_func.png}
    \caption{Coroutine calling a normal Function g()}
    \label{fig:CoroutineCallsNormalFunction}
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/coroutines_drawing4_other_func_returns.png}
    \caption{Normal Function g() returns}
    \label{fig:NormalFunctionReturns}
  \end{minipage}
\end{figure}

If c() now hits a supension point as shown in \autoref{fig:CoroSuspensionPoint}, the Suspend operation is invoked where c() suspends execution and returns control to f() without destroying its activation frame.
The Suspend operation interrupts execution of the coroutine at a current, well-defined point - co\_await or co\_yield -, within the function and potentially transfers execution back to the caller without destroying the activation frame.

This results in the stack-frame part of c() being popped off the stack while leaving the coroutine-frame on the heap.
When the coroutine suspends for the first time, a return-value is returned to the caller.
This return value often holds a handle to the coroutine-frame that suspended that can be used to later resume it.
When c() suspends it also stores the address of the resumption-point of c() in the coroutine frame (in the illustration called RP for resume-point).

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\paperwidth]{figures/coroutines_drawing5_coro_suspension_point.png}
  \caption{Coroutine hits Suspension Point}
  % \quelle{\cite{ADaSurvey}}
  \label{fig:CoroSuspensionPoint}
\end{figure}

The Resume operation transfers execution back to the coroutine at the point it was suspended whereas the Destroy operation is the only operation that destroys the activation frame of the coroutine.

This handle may now be passed around as a normal value between functions. At some point later, potentially from a different call-stack or even on a different thread, something (say, h()) will decide to resume execution of that coroutine. For example, when an async I/O operation completes.

The function that resumes the coroutine calls a void resume(handle) function to resume execution of the coroutine. To the caller, this looks just like any other normal call to a void-returning function with a single argument.

This creates a new stack-frame that records the return-address of the caller to resume(), activates the coroutine-frame by loading its address into a register and resumes execution of x() at the resume-point stored in the coroutine-frame.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\paperwidth]{figures/coroutines_drawing6_resumption_of_coroutine.png}
  \caption{Resumption of a Coroutine}
  % \quelle{\cite{ADaSurvey}}
  \label{fig:CoroResumption}
\end{figure}

\cite{baker2017coroutines}


%%% ------- Execution of a Coroutine ------------------------
\subsection{Execution of a Coroutine}\label{subsec:ExecutionOfACoroutine}
% https://en.cppreference.com/w/cpp/language/coroutines.html, and lewis baker

Each coroutine is associated with a promise object, a coroutine handle and the coroutine state.
% the promise object, manipulated from inside the coroutine. The coroutine submits its result or exception through this object. Promise objects are in no way related to std::promise.
% the coroutine handle, manipulated from outside the coroutine. This is a non-owning handle used to resume execution of the coroutine or to destroy the coroutine frame.
% the coroutine state, which is internal, dynamically-allocated storage (unless the allocation is optimized out), object that contains
% - the promise object
% - the parameters (all copied by value)
% - some representation of the current suspension point, so that a resume knows where to continue, and a destroy knows what local variables were in scope
% - local variables and temporaries whose lifetime spans the current suspension point.

As partially illustrated in \autoref{fig:CallingACoroutineHeap}, when a coroutine begins execution, it performs the following:
- allocates the coroutine state object using operator new and copies all function parameters to the coroutine state.
- calls the constructor for the promise object.
- calls promise.get\_return\_object() and stores the result in a local variable, which will be returned to the caller on the first suspension point of the coroutine.
- calls promise.initial\_suspend() and co\_awaits its result. Typically returns std::suspend\_always (lazily-started) or std::suspend\_never (eagerly-started).
- when co\_await promise.initial\_suspend() resumes, starts executing the body of the coroutine.

\begin{lstlisting}[language=C, frame=single, caption={coroutine execution}, label={lst:coroutine_execution}]
{
  co_await promise.initial_suspend();
  try
  {
    <body-statements>
  }
  catch (...)
  {
    promise.unhandled_exception();
  }
FinalSuspend:
  co_await promise.final_suspend();
}
\end{lstlisting}


When a coroutine reaches a suspension point, co\_await or co\_yield, the return object obtained earlier is returned to the caller/resumer. %, after implicit conversion to the return type of the coroutine, if necessary.
When encountering the co\_return statement, it calls either promise.return\_void()
or promise.return\_value(expr) depending on whether an expression is provided and whether this expression is non-void.
Furthermore, all variable with automatic storage duration are destroyed in reverse order of their creation and finally calling promise.final\_suspend() and co\_awaiting its result.
When falling off the end of the coroutine, meaning there is no co\_return statement, it is equivalent to a co\_return; statement.

% Returning from the coroutine using co_return
% When the coroutine reaches a co_return statement, it is translated into either a call to promise.return_void() or promise.return_value(<expr>) followed by a goto FinalSuspend;.

% The rules for the translation are as follows:

% co_return;
% -> promise.return_void();
% co_return <expr>;
% -> <expr>; promise.return_void(); if <expr> has type void
% -> promise.return_value(<expr>); if <expr> does not have type void
% The subsequent goto FinalSuspend; causes all local variables with automatic storage duration to be destructed in reverse order of construction before then evaluating co_await promise.final_suspend();.

% Note that if execution runs off the end of a coroutine without a co_return statement then this is equivalent to having a co_return; at the end of the function body. In this case, if the promise_type does not have a return_void() method then the behaviour is undefined.

% If either the evaluation of <expr> or the call to promise.return_void() or promise.return_value() throws an exception then the exception still propagates to promise.unhandled_exception() (see below).





%% uncaught expression
% If the coroutine ends with an uncaught exception, it performs the following:
% catches the exception and calls promise.unhandled_exception() from within the catch-block
% calls promise.final_suspend() and co_awaits the result (e.g. to resume a continuation or publish a result). It's undefined behavior to resume a coroutine from this point.
% When the coroutine state is destroyed either because it terminated via co_return or uncaught exception, or because it was destroyed via its handle, it does the following:
% calls the destructor of the promise object.
% calls the destructors of the function parameter copies.
% calls operator delete to free the memory used by the coroutine state.
% transfers execution back to the caller/resumer.
%% \paragraph{Dynamic allocation}
% Coroutine state is allocated dynamically via non-array operator new.
% The call to operator new can be optimized out (even if custom allocator is used) if The lifetime of the coroutine state is strictly nested within the lifetime of the caller, and
% the size of coroutine frame is known at the call site. In that case, coroutine state is embedded in the caller's stack frame (if the caller is an ordinary function) or coroutine state (if the caller is a coroutine).
% If allocation fails, the coroutine throws std::bad\_alloc, unless the Promise type defines the member function Promise::get\_return\_object\_on\_allocation\_failure(). If that member function is defined, allocation uses the nothrow form of operator new and on allocation failure, the coroutine immediately returns the object obtained from Promise::get\_return\_object\_on\_allocation\_failure() to the caller, e.g.:

%%% ------- Implementation details of a coroutine ------------------------
\subsection{Implementation details/concepts of a coroutine}\label{subsec:ImplementationOfACoroutine}
% \begin{lstlisting}[language=C, frame=single, caption={minimum coroutine co_yield example}, label={lst:minimum_coroutine_co_yield_example}]
%   struct Task {
%     struct promise_type {
%       int _val{};
%       Task get_return_object() { return Task{this}; }
%       std::suspend_never initial_suspend() { return {}; }
%       std::suspend_always final_suspend() noexcept { return {}; }
%       std::suspend_always yield_value(int value) { _val = value; return {}; }
%       void unhandled_exception() { }
%       };

%       std::coroutine_handle<promise_type> h{};  // coroutine handle

%       explicit Task(promise_type* p) : h{std::coroutine_handle<promise_type>::from_promise(*p)}{}
%       Task(Task&& rhs) : h{std::exchange(rhs.h, nullptr)} { }
%       ~Task() { if (h) { h.destroy(); } }
% };
% Task Coroutine() {
%     for (int i = 0; i < 5; ++i) {
%       co_yield i;
%     }
% }
% \end{lstlisting}
The following is a minimum coroutine example using co\_await, illustrated in \autoref{lst:minimum_coroutine_co_await_example}.
The Task struct is a the coroutine wrapper holding the coroutine handle and the promise\_type struct defining the promise type and thus, the behavior of the coroutine.
The function Task Coroutine(int num\_steps) is the coroutine function containing a co\_await expression suspending the coroutine for num\_steps times.
And as explained in previous subsection, also see \autoref{lst:coroutine_execution}, when another function, e.g. main(), calls the coroutine, the coroutine frame is allocated on the heap, the promise object is constructed and initial\_suspend() is called.
When the coroutine hits the co\_await expression, it suspends execution and returns control to the caller until it is resumed again.
The coroutine results in the output of "Coroutine at step i" and "Resuming coroutine" for num\_steps times, in this case from 0 to 4, as initial\_suspend() is set to suspend\_never and thus, the coroutine starts executing immediately when called.
If it it set to suspend\_always, the coroutine initially pauses before starting work (lazily computed coroutine) and results in "Resuming coroutine" being printed first followed by "Coroutine at step i"  num\_steps times.
\begin{lstlisting}[language=C, frame=single, caption={minimum coroutine co\_await example}, label={lst:minimum_coroutine_co_await_example}]
  struct Task {
    struct promise_type {
        Task get_return_object() { return Task{this}; }
        std::suspend_always initial_suspend() { return {}; }
        std::suspend_never final_suspend() noexcept { return {}; }
        void unhandled_exception() { }
        void return_void() { }
      };

      std::coroutine_handle<promise_type> h{};  // coroutine handle

      explicit Task(promise_type* p) : h{std::coroutine_handle<promise_type>::from_promise(*p)}{}
      Task(Task&& rhs) : h{std::exchange(rhs.h, nullptr)} { }
      ~Task() { if (h) { h.destroy(); } }
};
Task PrintCoroStep(int num_steps) {
    for (int i = 0; i < num_steps; ++i) {
      std::cout << "Coroutine at step " << i << "\n";
      co_await std::suspend_always{};  // if suspend_never, coro would never pause
    }
    co_return;  // equivalent to omitting this line
}
int main() {
    Task task = PrintCoroStep(5);
    for (int i = 0; i < 5; ++i) {
      std::cout << "Resuming coroutine\n";
      task.h.resume();
    }
}
\end{lstlisting}
In this simple example, trivial awaitables std::suspend\_always and std::suspend\_never, defined in the standard library, are used to control the suspension behavior of the coroutine at the initial and co\_await suspension points.
The implementation of suspend\_always is shown in \autoref{lst:suspend_always} returning false in await\_ready(), indicating that the await expression always suspends and waits for a value (is not ready).
The suspend\_never implementation is analog with the only difference of returning true in await\_ready(), indicating that the await expression never suspends (is always ready).
\begin{lstlisting}[language=C, frame=single, caption={suspend\_always implementation}, label={lst:suspend_always}]
struct suspend_always {
  constexpr bool await_ready() const noexcept{return false;}
  constexpr void await_suspend(std::coroutine_handle <>) const noexcept {}
  constexpr void await_resume() const noexcept {}
};
\end{lstlisting}

For more customization, more complex awaitabletypes can be implemented, requiring to implement these three methods listed in \autoref{lst:awaitable_type}.
\begin{lstlisting}[language=C, frame=single, caption={awaitable type}, label={lst:awaitable_type}]
  bool await_ready();

  // one of:
  void await_suspend(std::coroutine_handle<> h);
  bool await_suspend(std::coroutine_handle<> h);
  std::coroutine_handle<> await_suspend(std::coroutine_handle<> h);

  T await_resume();
\end{lstlisting}

In the $ co\_await expr; $ operator the expression, the awaiter type, is firstly converted to an awaitable.
If the promise\_type type of the current coroutine has a member function await\_transform, then the awaitable is obtained by calling promise.await\_transform(expr), illustrated in \autoref{lst:pseudo_code_which_awaiter_awaitable}.
Otherwise, the awaitable is expr as-is.

\begin{lstlisting}[language=Python, frame=single, caption={Pseudo Code for deciding which awaiter/awaitable is used}, label={lst:pseudo_code_which_awaiter_awaitable}]
func get_awaitable(promise_type& promise, T&& expr){
  if P has member function await_transform:
    return promise.await_transform(expr);
  else
    return expr;
}

func get_awaiter(awaitable){
  if awaitable has member operator co_await:
    return awaitable.operator co_await();
  else if awaitable has non-member operator co_await:
    return operator co_await(awaitable);
  else:
    return awaitable;
}
\end{lstlisting}

Then, the awaiter object is obtained like illustrated in func get\_awaiter in \autoref{lst:pseudo_code_which_awaiter_awaitable}.
If no operator co\_await is defined, the awaitable itself is the awaiter which implicates that a type can be an awaitable and an awaiter type simultaneously.

Then, awaiter.await\_ready() is called , The coroutine is suspended awaiter.await\_suspend(handle) is called, where handle is the coroutine handle representing the current coroutine.
Inside that function, the suspended coroutine state is observable via that handle,
if await\_suspend returns void, control is immediately returned to the caller/resumer of the current coroutine (this coroutine remains suspended), otherwise
if await\_suspend returns bool, the value true returns control to the caller/resumer of the current coroutine the value false resumes the current coroutine.
if await\_suspend returns a coroutine handle for some other coroutine, that handle is resumed (by a call to handle.resume())
Finally, awaiter.await\_resume() is called (whether the coroutine was suspended or not), and its result is the result of the whole co\_await expr expression.

If the coroutine was suspended in the co\_await expression, and is later resumed, the resume point is immediately before the call to awaiter.await\_resume().

Additionally to suspending the coroutine, the co\_yield expression returns a value to the caller and is
equivalent to co\_await promise.yield\_value(expr).
However, to define the type to return, the promise\_type has to implement the yield\_value() function.


Besides the awaiter/awaitable concept explained in the previous paragraphs, there is also the promise type concept.
These are the two main interfaces, defined by the coroutine Type Specification (TS), for customizing the behavior of coroutines and co\_await expressions:
\begin{itemize}
  \item Awaiter / Awaitable: specifies methods that controls the semantics of co\_await expression. When a value is co\_awaited, the awaitable object allows to specify whether to suspend,
   execute some logic after suspensions (for asynchronously completed operations) and/or execute some logic after the coroutine resumes.
  \item Promise Type: specifies methods for customising the behavior of a coroutine, e.g. the behavior of any co\_await or co\_yield expression inside the coroutine body.
\end{itemize}


The promise\_type struct has to be implemented in the coroutine wrapper type, which can be named arbitrarily, e.g. Task in \autoref{lst:minimum_coroutine_co_await_example}.
It has to follow the scheme illustrated in \autoref{lst:promise_type}.
% A coroutine in C++ is an finite state machine (FSM) that can be controlled and customized by the promise_type.
\begin{lstlisting}[language=C, frame=single, caption={Promise Type}, label={lst:promise_type}]
struct promise_type{
  // required methods
  ReturnType get_return_object();
  std::suspend_always initial_suspend();
  std::suspend_always final_suspend() noexcept;
  void unhandled_exception();

  // depending on ReturnType
  void return_void(); // if ReturnType is void
  void return_value(T value); // if ReturnType is T

  // optional methods
  auto await_transform(U&& value); // customize co_await behavior
  auto yield_value(V value); // customize co_yield behavior
}
\end{lstlisting}

The Promise type is determined by the compiler from the return type of the coroutine using std::coroutine\_traits.

As mentioned in \autoref{subsec:ExecutionOfACoroutine}, each coroutine is associated with a coroutine handle.
This handle is a non-owning reference to the coroutine frame and can be used to resume execution of the coroutine or to destroy the coroutine frame.

% \begin{lstlisting}[language=C, frame=single, caption={coroutine chat with await\_transform example}, label={lst:coroutine_chat_await_transform_example}]
% // #include <coroutine> #include <iostream> #include <utility>
% struct Chat {
%     struct promise_type {
%         std::string _msgOut{}, _msgIn{};

%         void unhandled_exception()noexcept{};
%         Chat get_return_object() {return Chat{this};}
%         std::suspend_always initial_suspend() { return {}; }
%         std::suspend_always final_suspend() noexcept { return {}; }
%         std::suspend_always yield_value(std::string msg) {
%           _msgOut = std::move(msg); return {};}
%         void return_value(std::string msg) noexcept {
%           _msgOut = std::move(msg); }
%         auto await_transform(std::string) noexcept {
%             struct awaiter {
%                 promise_type& pt;
%                 constexpr bool await_ready() const noexcept {return true;}
%                 void await_suspend(std::coroutine_handle<>)const noexcept{}
%                 std::string await_resume() const noexcept {
%                   return std::move(pt._msgIn); }
%             };
%             return awaiter{*this};
%         }
%     };
%     std::coroutine_handle<promise_type> _hdl;
%     explicit Chat(promise_type* p) : _hdl{
%       std::coroutine_handle<promise_type>::from_promise(*p)}{}
%     Chat(Chat&& rhs) : _hdl{std::exchange(rhs._hdl, nullptr)} {}
%     ~Chat() {if (_hdl) {_hdl.destroy();}}

%     std::string listen() {
%         if(not _hdl.done()) {_hdl.resume();}
%         return std::move(_hdl.promise()._msgOut);}
%     void answer(std::string msg) {
%         _hdl.promise()._msgIn = msg;
%         if(not _hdl.done()) {_hdl.resume();}}
% };

% Chat Fun(){
%     co_yield "Hello\n";
%     std::cout << co_await std::string{};
%     co_return "Here!\n";}
% int main(){
%     Chat chat = Fun();
%     std::cout << chat.listen();
%     chat.answer("Where are you?\n");
%     std::cout << chat.listen();
% }
% \end{lstlisting}


\cite{baker2018understanding}
\cite{baker2018promise}


% %%% ------ Comparison to other alternative solutions to hide cache misses ------------------------
% \subsection{Comparison to other alternative solutions to hide cache misses}
% AMAC, Group Prefetching
% coro is better, not performance wise but easier to use, more general


%%% ====== CACHE MISSES =======================
\section{Cache Misses}\label{section:CacheMisses}
This section provides a background on cache misses, why they cause latencies in modern computer architectures by accessing data
and how they can be potentially be hidden.

\subsection{Definition of Cache Misses}\label{subsec:DefinitionOfCacheMisses}



To explain cache misses, first the underlying concept of caches in modern computer architectures has to be explained briefly.
Caches are small, fast memory units located close to the CPU cores, designed to store frequently accessed data and instructions to reduce the average time to access memory.
When the CPU needs to read or write data, it first checks if the data is present in the cache (a cache hit).
Typically, there are three levels of caches (L1, L2, L3 or called LLC) with L1 being the smallest and fastest, and L3 being the largest and slowest.
If the data is not found in the caches (a cache miss), it has to be fetched from the slower main memory, which incurs a significant performance penalty.
% picture of memory hierarchy

Cache misses are typically categorized into three main types: compulsory misses, capacity misses, and conflict misses.
Compulsory misses occur on the first access to a block, as the data has not yet been loaded into the cache regardless of cache design.
Capacity misses happen when the cache can not hold all the data actively used by a program during execution, meaning the working set exceeds the cache size.
Conflict misses, also known as interference misses, occur when multiple memory addresses map to the same cache set, leading to evictions of useful data even though the cache is not full.
For a multi-processor system this 3Cs group of cache misses can be extended to 4Cs.
The 4th C are coherence misses arising from a cache line being invalidated due to modifications made by other processors (for L3 Cache as it is typically shared across cores) or threads (for L1, L2 and L3 Cache).

% Research and literature have explored various strategies to mitigate cache misses. Increasing cache capacity can reduce both capacity and conflict misses, although it does not affect
% compulsory misses.
% Adjusting block size can also impact miss rates; larger blocks may reduce compulsory misses due to spatial locality but can increase conflict misses by reducing the number of sets in a
% fixed-size cache.
% Techniques such as loop nest optimization and virtual coloring are used in software to minimize conflict misses by ensuring that frequently accessed data does not map to the same cache set.
% Additionally, cache replacement policies like LRU (Least Recently Used) and write strategies such as write-back or write-through are designed to manage data in the cache efficiently,
% with the goal of minimizing the number of misses and their associated penalties.


% % WRITE MISS
% This is an old question, but I don't think any of the other answers quite explain what the text is saying.
% A word is typically smaller than a cache line; a system might, for example, have words that are 8 bytes in size,
% but L1 cache lines that are 64 bytes in size. A write of a word means overwriting some portion of that cache line, not the whole cache line.
% Therefore, to perform such a write, you would typically need to load the rest of the 64-byte aligned line into cache, so that just the modified portion can be overwritten.
% Another issue that the text doesn't mention, but is important in practice, is cache coherency.
% On many multiprocessing architectures, memory semantics demands that every CPU sees every write occur in the same order.
% Those that have more relaxed semantics still need some kind of memory sequencing otherwise CPUs can't synchronise.
% The upshot is that a CPU often must gain some kind of exclusive write access to a cache line before it is allowed to write.
% So even if a CPU has a copy of the cache line in its L1 cache, it may need to acquire that access, which is also a kind of write miss.

%  Computer Organization and Design MIPS Edition 5th Edition The Hardware/Software Interface



\subsection{Cause of Cache Misses in Data Structures}\label{subsec:CauseOfCacheMissesInDataStructures}

% Pointer Chasing. Modern database engines use various inmemory data structures that are directly addressed by virtual memory pointers. Memory blocks used by these data structures are
% usually allocated from the heap and chained together using pointers. For example, the nodes of an in-memory B+-tree are allocated
% and deallocated as the tree grows and shrinks. To traverse a tree
% from its root node to the target leaf node, the accessing thread
% must dereference multiple pointers from the root to the leaf node,
% forming a random access pattern. Unfortunately, such patterns are
% very difficult (if not impossible) for hardware prefetchers to predict accurately, leading to very high likelihood of last-level cache
% misses1 upon pointer dereference. As a result, in main-memory systems data stalls often dominate total CPU execution time. Beyond
% indexes, version chains in multi-versioned systems as described in
% Section 2.1 can also cause a significant amount of data stalls when
% the accessing thread searches for a particular version. In some systems, over 50% of the total CPU cycles can be spent on waiting for
% data to be loaded from memory to CPU caches [21]. Hiding such
% stalls can potentially lead to much higher overall performance.

\subsection{Techniques to counter Cache Misses}
% 2.3 Tackling Cache Misses
% In the literature, we find many software techniques to deal
% with cache misses. Based on how they affect the number of
% cache misses and the incurred penalty, these techniques fall
% into the following three categories:
% • Eliminate cache misses by increasing spatial and
% temporal locality. Locality is increased by a) eliminating indirection, designing cache-conscious data structures like the CSB+-tree [28]; b) matching the data
% layout to the access pattern of the algorithm, i.e, store
% data that are accessed together in contiguous space; or
% c) reorganizing memory accesses to increase locality,
% e.g., with array [16] and tree blocking [14, 32]. In this
% work, we assume that the index has the best possible
% implementation and locality cannot be further increased without penalizing single lookups. Nonetheless,
% our proposal can be applied to any index structure.
% • Reduce the cache miss penalty by scheduling independent instructions to execute after a load; this
% approach increases instruction-level parallelism and leads to more effective out-of-order execution. To reduce
% the main-memory access penalty, a non-blocking load
% has to be introduced early enough, allowing independent instructions to execute while fetching data. This
% is achieved through simple prefetching within an instruction stream, or exploiting simultaneous multithreading with helper threads that prefetch data [31]. In
% index lookups, however, one memory access depends
% on the previous one with few independent instructions
% in-between, so these techniques do not apply.
% • Hide the cache miss penalty by overlapping memory accesses. The memory system can serve several
% memory requests in parallel (10 in current Intel CPUs)
% and exploiting this memory-level parallelism increases
% memory throughput. However, overlapping requires
% independent memory accesses, which do not exist in
% the access chain of an index lookup.
% Takeaway. These approaches do not benefit individual
% index lookups. Next, we show how to hide the cache misses
% of a group of lookups with interleaved execution.


\subsection{Lifecycle of a prefetch instruction through the memory subsystem}\label{subsec:LifecyclePrefetchInstruction}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\paperwidth]{figures/prefetch_instructions_lifecycle_through_memory_subsystem.png}
  \caption{Prefetch Lifecycle through memory subsystem}
  % \quelle{\cite{ADaSurvey}}
  \label{fig:PrefetchLifecycleThroughMemorySubsystem}
\end{figure}











% At first glance, the concept of software-based prefetching appears
% seemingly straightforward: data accesses, which are challenging
% or unfeasible for the hardware to predict, can be communicated
% to the CPU via specific instructions—interpreted as hints from the
% application. Based on these hints, the underlying hardware substrate can move data asynchronously into a cache close to the CPU
% to minimize the access latency—hiding the actual latency behind
% computational work. Data structures that exhibit indirect access
% patterns provide good examples for instructing the hardware with
% insights from the software layer, for instance, linked lists, queues,
% hash tables, and trees.
% 2.1 Utilize Software Prefetching
% However, formulating such predictions is often cumbersome. Consider the traversal through a tree-like data structure: The immediate
% succession of identifying and accessing a node leaves negligible
% time for the hardware to load the data, even if the software hints
% about the soon-to-be-accessed tree node. In order to create a (sufficient) temporal gap between pinpointing the next node and accessing it, the literature explores various methods. The key idea
% is to break operations into stages and use pipelining, overlaying
% the prefetch of one item with the compute of others, for example, by processing operations in groups [9, 18]. A more general
% approach is to utilize asynchronous control flow abstractions, such
% as coroutines [11, 16, 30, 37] and fine-grained tasks [28]. In addition,
% the discussion over the strategic implementation of optimization
% passes to automatically inject prefetch instructions into compilergenerated code has been ongoing for decades (e.g., [6, 15, 22, 27]).
% Not only does the redesign of algorithms and data structures
% play a decisive role in the efficient use of software prefetching, but
% the instruction must also be issued at the appropriate moment [15].
% When software executes the prefetching instruction too early before the actual access, the data may have been already evicted from
% the cache when needed. Conversely, if prefetched too late, the data
% might not be transferred to the cache completely, potentially causing the CPU to wait. Accurate prefetch timing necessitates careful
% understanding of both the execution time of the instructions preceding the actual load and the system’s memory latency [6, 27].
% However, the presence of intricate memory systems like NUMA
% and high-bandwidth memory, as well as the increasing sophistication of CPUs, which now feature out-of-order execution and
% superscalar architectures, exacerbate the complexity of achieving
% precise timing. A promising approach to tackle these challenges is
% the implementation of profile-guided approaches [15].
% TLBs L1d LFB L2 off-core
% prefetchNTA [0xA]
% prefetchT0 [0xB]
% prefetchT1 [0xC]
% synchronous
% asynchronous
% Figure 2: Lifecycle of various prefetch instructions through
% the memory subsystem.
% 2.2 Lifecycle of a Prefetch
% Almost all modern server and desktop platforms offer several prefetch
% instructions to target different levels of the cache hierarchy. Notably, each instruction will initiate the transfer for one cache line.
% The implementation within various hardware systems has only
% slight variations, even across different vendors. When the software
% executes a prefetch, the logical address to be prefetched is translated
% first into a physical one. This procedure happens synchronously, i.e.,
% if the translation is not cached within the Translation Lookaside
% Buffer (TLB), the CPU will contact a Page Miss Handler (PMH)
% before continuing the execution of the prefetch instruction. We
% visualize the lifecycle of prefetch requests through the memory
% subsystem in Figure 2.
% Once the physical address is known and the request misses the
% L1 data cache (L1d), the hardware will trigger the transfer from
% memory to caches asynchronously. To that end, the L1d cache requests the cache line through the Line Fill Buffer (LFB) (or Miss
% Address Buffer (MAB) on AMD platforms1
% ). The LFB, as suggested
% by its name, acts as a buffer-like structure that interfaces between
% the L1d and L2 cache to communicate requests on a cache-line
% granularity [8, 33, 34]. For every cache line that misses the L1d,
% the CPU allocates a slot in the LFB. The L2 cache, on the opposite
% end, retrieves and delivers the requested lines. This nature makes
% it straightforward to implement asynchronous prefetches: The desired cache line will not cause the instruction to wait until the
% data is transferred into the cache but only until the miss is communicated to the LFB. Furthermore, the LFB enables the CPU to
% handle several pending requests simultaneously without blocking
% a single one, supporting out-of-order execution and performing
% micro-optimizations like merging multiple requests to the same
% cache line [34]. When the request also misses the L2 cache, it seeks
% the data from even higher cache levels or main memory. On Intel
% platforms, the superqueue [19]—a buffer positioned between the
% L2 and off-core last-level caches (LLCs)—tracks pending requests.
% Upon completing a memory request, the specific software prefetch instruction dictates how close the data is moved to the CPU.
% In the x86 and ARM ISAs, most prefetch instructions clearly specify
% the cache level target for the fetched data: Cache lines fetched by
% prefetcht1 (and its counterpart pldl2keep on ARM) are stored in
% the L2 cache; prefetcht0 (pldl1keep on ARM) moves data further
% into the L1d, thereby passing through the LFB. The prefetcht2
% instruction is generally aimed to fetch data into the LLC. However,
% recent Intel platforms (since Skylake) have implemented the LLC as
% a non-inclusive victim cache—in that case, data is placed into the L2
% instead. Conversely, for the non-temporal prefetchnta instruction,
% the x86 ISA does not specify a particular cache target but only the
% 1
% For the sake of simplicity, we only ever refer to the LFB, but also intend the MAB.
% How to Be Fast and Not Furious: Looking Under the Hood of CPU Cache Prefetching DaMoN ’24, June 10, 2024, Santiago, AA, Chile
% objective: reducing cache pollution for data that will be accessed
% nonrecurring.
% AMD’s documentation for earlier processor generations (e.g.,
% Bulldozer [3]) specifies that the prefetchnta instruction pulls data
% into the L1d and ensures it is not evicted to the L2 cache unless
% it originated from there. For later generations (i.e., for Zen), the
% documentation no longer mentions the exact cache destination but
% merely states that the L2 is bypassed when evicting non-temporal
% data. Intel, in contrast, also specifies for modern platforms that
% non-temporal prefetches move data into the L1d and into the LLC
% if the LLC is inclusive [13]. Both Intel and AMD note that these
% non-temporally prefetched cache lines are prioritized for quicker
% eviction [4, 13].
% 2.3 Limitations
% Modern platforms exhibit two notable limitations in the implementation of software prefetches. First, the execution of a single
% prefetch instruction stalls until the virtual address is translated
% into a physical address [13, 30]. Although the TLB might accelerate
% this process, it is likely that the most prefetched addresses are not
% present in the TLB as software prefetching is used primarily when
% the application accesses scattered data objects. Consequently, the
% latency of the prefetch instruction is dominated by the latency of
% the PMH performing a page-table walk to translate the address.
% However, this restriction only applies to the initial prefetch operation within a memory page if several prefetches are performed
% consecutively to load a block larger than a single cache line. While
% hardware also implements prefetchers for address translations [35],
% the software is only empowered to preload data.
% Drawing from that observation, it seems advantageous to use
% prefetching for larger, contiguous data regions by executing multiple instructions while only experiencing a single TLB miss per
% page. However, this may introduce a secondary constraint: Due to
% the limited number of LFB entries (typically between 3 and 24), the
% hardware can accommodate only a small number of outstanding
% memory requests. If an excessive volume of prefetch requests is
% sent out rapidly, the LFB becomes filled and cannot accept further
% requests. Under this circumstance, the CPU will either stall until
% an LFB slot becomes ready or—according to Intel’s documentation [13]—drop new prefetch requests.






% https://dl.acm.org/doi/epdf/10.1145/2133382.2133384




% %%% ====== COMPUTER ARCHITECTURE =======================
% \section{Computer Architecture}\label{section:ComputerArchitecture}

% \subsection{Different Computer Architectures, x86-64, amd, apple, mips}

% \subsection{Introduction to computer achitecture and storage layout}
% pyramid, and tlb access schemata

% \subsection{Different Compilers - Gnu vs Clang}

% % There are multiple C compilers due to a combination of historical development, hardware and software specialization, and differing design philosophies. The need for compilers optimized for specific hardware and operating systems has driven the creation of diverse tools. For example, Intel's ICC (Intel C Compiler) is optimized for Intel processors, while AMD's AOCC (AMD Optimizing C/C++ Compiler) is tailored for AMD systems, aiming to maximize performance on their respective architectures.
% %  Similarly, compilers like ACK were developed specifically for operating systems such as OpenBSD, highlighting the role of the target environment in compiler design.

% % The evolution of the C language itself, with multiple standards (C89, C99, C11, C17, C23), has also contributed to diversity. Some compilers support newer language features more comprehensively than others, and certain compilers may not support all C standards equally.
% %  This leads to a need for multiple compilers to ensure code compatibility and performance across different platforms and use cases.

% % Additionally, the design and architecture of compilers vary significantly. GCC (GNU Compiler Collection) is a monolithic system with a complex internal structure, while Clang, part of the LLVM project, was designed from the start as a modular API, making it easier to integrate into tools like IDEs, static analyzers, and refactoring utilities.
% %  This architectural difference influences how compilers are used and extended.

% % Despite the existence of many compilers, only a few are widely used in practice. The major players include GCC, Clang, and Microsoft Visual C++ (MSVC), which dominate due to their robustness, extensive feature sets, and strong community support.
% %  However, niche compilers like Pelles C, LCC, and TCC (Tiny C Compiler) exist for specific purposes such as embedded development or minimal footprint requirements.
% %  Some compilers, like the Portable C Compiler or LabWindows/CVI, are used by specialized developer communities.

% % The presence of multiple compilers also stems from the fact that C and C++ are closely related, with C++ being largely a superset of C. As a result, many compilers that support C++ also support C, and the effort to build a C++ compiler often includes C support as a natural byproduct.
% %  This shared foundation reduces the marginal cost of supporting both languages, but it does not eliminate the need for specialized tools optimized for particular environments or performance goals.

% % file://wsl.localhost/Ubuntu/home/danieldagruber/TUM_Master_11/00_MasterThesis/07_p577-huang_TheArtOfLatencyHidingInModernDatabaseEngines.pdf
% % where can latencies come from: pointer chasing, synch, storage io, oversubscription, osschedling
% % https://chipsandcheese.com/p/skylake-intels-longest-serving-architecture
