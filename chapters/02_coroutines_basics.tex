% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.



% • Describing different methods
% • Indicating a specific method
% • Giving reasons why a particular method was adopted
% • Indicating sample size and characteristics
% • Indicating reasons for sample characteristics
% • Describing the process: Indicating problems or limitations


%%% ====== COROUTINES BASICS =======================
\chapter{Coroutines and Cache Misses Fundamentals}\label{chapter:CacheCoroDSFundamentals}

%%% ====== COROUTINES =======================
\section{C++ Coroutines}\label{sec:CppCoroutines}
% \section{C++ Coroutines Coroutine CPP Reference standardisation history}\label{section:Coroutines}
% Citation test~\parencite{latex}.

%%% ------- Intro to COROUTINES ------------------------
\subsection{Introduction to Coroutines}\label{subsec:IntroductionToCoroutines}
C++ Coroutines are available in the std namespace from the C++20 standard on, and from the C++23 standard there is a generator implementation based on the C++ 20 coroutine concept.
% https://en.cppreference.com/w/cpp/language/coroutines

A coroutine is a generalization of a function, being able additionally to normal functions to suspended execution and resumed it later on, as illustrated in \autoref{fig:function_vs_coroutine_execution}.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\paperwidth]{figures/function_vs_coroutine_execution.png}
  \caption{Function vs Coroutine Execution}
  % \quelle{\cite{ADaSurvey}}
  \label{fig:function_vs_coroutine_execution}
\end{figure}
Any function is a coroutine if its definition contains at least one of these three keywords:
\begin{itemize}
  \item co\_await - to suspend execution
  \item co\_yield - to suspend execution and returning a value
  \item co\_return - to complete exeuction and returning a value
\end{itemize}
C++ Coroutines are stackless, meaning by suspension and consecutively returning to the caller, the data of the coroutine is stored separately from the stack, namely on the heap.
This allows sequential code to be executed asynchronously, without blocking the thread of execution and supports algorithms to be lazily computed, e.g. generators.
However, there are some restrictions to coroutines: They cannot use variadic arguments, plain return statements or placeholder return types likes auto or Concept.
Also consteval, conexpr and the main function as well as constructors and destructors cannot be coroutines.

To illustrate the difference between coroutines and functions, this paragraph provides a background and general information of function calls and return:
A normal function has a single entry point - the Call operation - and a single exit point - the Return operation.
The Call operation creates an activation frame, suspends execution of the caller and transfers execution to the callee, where the caller is the invocating function and the callee is the invocated function.
The Return operation returns the value in the return statement to the caller, destroys the activation frame and then resumes execution of the caller.
These operations include calling conventions splitting the responsibilites of the caller and callee regarding saving register values to their activation frames.
The activation frame is also commonly called stack frame, as the functions state (parameters, local variables) are stored on the stack.
Normal Functions have strictly nested lifetimes, meaning they run synchronously from start to finish, allowing the stack to be a highly efficient memory allocation data-structure for allocation and freeing frames.
The pointer pointing at the top of the stack is the **rsp** register on X86-64 CPU Architectures.

Coroutines have, additionally to the call and return operation, three extra operations, namely suspend, resume and destroy.
As coroutines can suspend execution without destroying the activation frame, as it may be resumed later, the activation frames are not strictly nested anymore.
This requires that after the creation of the coroutine on the stack, the state of the coroutine is saved to the heap,
like illustrated in \autoref{fig:CallingACoroutineHeap} where a normal function f() calls a coroutine function c().
% https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2017/n4680.pdf

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\paperwidth]{figures/coroutines_drawing2_coro_call_heap.png}
  \caption{Calling a Coroutine - Heap Allocation}
  % \quelle{\cite{ADaSurvey}}
  \label{fig:CallingACoroutineHeap}
\end{figure}

If the coroutine calls another normal function g(), g() 's activation frame is created on the stack and the coroutine stack frame points to the heap allocated frame,
 as illustrated in \autoref{fig:CoroutineCallsNormalFunction}. When g() returns, it destroys its activation frame and restores c()'s activation frame.
\begin{figure}[h]
  \begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/coroutines_drawing3_coro_calls_other_func.png}
    \caption{Coroutine calling a normal Function g()}
    \label{fig:CoroutineCallsNormalFunction}
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/coroutines_drawing4_other_func_returns.png}
    \caption{Normal Function g() returns}
    \label{fig:NormalFunctionReturns}
  \end{minipage}
\end{figure}

If c() now hits a supension point as shown in \autoref{fig:CoroSuspensionPoint}, the Suspend operation is invoked where c() suspends execution and returns control to f() without destroying its activation frame.
The Suspend operation interrupts execution of the coroutine at a current, well-defined point - co\_await or co\_yield -, within the function and potentially transfers execution back to the caller without destroying the activation frame.

This results in the stack-frame part of c() being popped off the stack while leaving the coroutine-frame on the heap.
When the coroutine suspends for the first time, a return-value is returned to the caller.
This return value often holds a handle to the coroutine-frame that suspended that can be used to later resume it.
When c() suspends it also stores the address of the resumption-point of c() in the coroutine frame (in the illustration called RP for resume-point).

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\paperwidth]{figures/coroutines_drawing5_coro_suspension_point.png}
  \caption{Coroutine hits Suspension Point}
  % \quelle{\cite{ADaSurvey}}
  \label{fig:CoroSuspensionPoint}
\end{figure}

The Resume operation transfers execution back to the coroutine at the point it was suspended whereas the Destroy operation is the only operation that destroys the activation frame of the coroutine.

This handle may now be passed around as a normal value between functions. At some point later, potentially from a different call-stack or even on a different thread, something (say, h()) will decide to resume execution of that coroutine. For example, when an async I/O operation completes.

The function that resumes the coroutine calls a void resume(handle) function to resume execution of the coroutine. To the caller, this looks just like any other normal call to a void-returning function with a single argument.

This creates a new stack-frame that records the return-address of the caller to resume(), activates the coroutine-frame by loading its address into a register and resumes execution of x() at the resume-point stored in the coroutine-frame.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\paperwidth]{figures/coroutines_drawing6_resumption_of_coroutine.png}
  \caption{Resumption of a Coroutine}
  % \quelle{\cite{ADaSurvey}}
  \label{fig:CoroResumption}
\end{figure}

\cite{baker2017coroutines}


%%% ------- Execution of a Coroutine ------------------------
\subsection{Execution of a Coroutine}\label{subsec:ExecutionOfACoroutine}
% https://en.cppreference.com/w/cpp/language/coroutines.html, and lewis baker

Each coroutine is associated with a promise object, a coroutine handle and the coroutine state.
% the promise object, manipulated from inside the coroutine. The coroutine submits its result or exception through this object. Promise objects are in no way related to std::promise.
% the coroutine handle, manipulated from outside the coroutine. This is a non-owning handle used to resume execution of the coroutine or to destroy the coroutine frame.
% the coroutine state, which is internal, dynamically-allocated storage (unless the allocation is optimized out), object that contains
% - the promise object
% - the parameters (all copied by value)
% - some representation of the current suspension point, so that a resume knows where to continue, and a destroy knows what local variables were in scope
% - local variables and temporaries whose lifetime spans the current suspension point.

As partially illustrated in \autoref{fig:CallingACoroutineHeap}, when a coroutine begins execution, it performs the following:
- allocates the coroutine state object using operator new and copies all function parameters to the coroutine state.
- calls the constructor for the promise object.
- calls promise.get\_return\_object() and stores the result in a local variable, which will be returned to the caller on the first suspension point of the coroutine.
- calls promise.initial\_suspend() and co\_awaits its result. Typically returns std::suspend\_always (lazily-started) or std::suspend\_never (eagerly-started).
- when co\_await promise.initial\_suspend() resumes, starts executing the body of the coroutine.

\begin{lstlisting}[language=C, frame=single, caption={coroutine execution}, label={lst:coroutine_execution}]
{
  co_await promise.initial_suspend();
  try
  {
    <body-statements>
  }
  catch (...)
  {
    promise.unhandled_exception();
  }
FinalSuspend:
  co_await promise.final_suspend();
}
\end{lstlisting}


When a coroutine reaches a suspension point, co\_await or co\_yield, the return object obtained earlier is returned to the caller/resumer. %, after implicit conversion to the return type of the coroutine, if necessary.
When encountering the co\_return statement, it calls either promise.return\_void()
or promise.return\_value(expr) depending on whether an expression is provided and whether this expression is non-void.
Furthermore, all variable with automatic storage duration are destroyed in reverse order of their creation and finally calling promise.final\_suspend() and co\_awaiting its result.
When falling off the end of the coroutine, meaning there is no co\_return statement, it is equivalent to a co\_return; statement.

% Returning from the coroutine using co_return
% When the coroutine reaches a co_return statement, it is translated into either a call to promise.return_void() or promise.return_value(<expr>) followed by a goto FinalSuspend;.

% The rules for the translation are as follows:

% co_return;
% -> promise.return_void();
% co_return <expr>;
% -> <expr>; promise.return_void(); if <expr> has type void
% -> promise.return_value(<expr>); if <expr> does not have type void
% The subsequent goto FinalSuspend; causes all local variables with automatic storage duration to be destructed in reverse order of construction before then evaluating co_await promise.final_suspend();.

% Note that if execution runs off the end of a coroutine without a co_return statement then this is equivalent to having a co_return; at the end of the function body. In this case, if the promise_type does not have a return_void() method then the behaviour is undefined.

% If either the evaluation of <expr> or the call to promise.return_void() or promise.return_value() throws an exception then the exception still propagates to promise.unhandled_exception() (see below).





%% uncaught expression
% If the coroutine ends with an uncaught exception, it performs the following:
% catches the exception and calls promise.unhandled_exception() from within the catch-block
% calls promise.final_suspend() and co_awaits the result (e.g. to resume a continuation or publish a result). It's undefined behavior to resume a coroutine from this point.
% When the coroutine state is destroyed either because it terminated via co_return or uncaught exception, or because it was destroyed via its handle, it does the following:
% calls the destructor of the promise object.
% calls the destructors of the function parameter copies.
% calls operator delete to free the memory used by the coroutine state.
% transfers execution back to the caller/resumer.
%% \paragraph{Dynamic allocation}
% Coroutine state is allocated dynamically via non-array operator new.
% The call to operator new can be optimized out (even if custom allocator is used) if The lifetime of the coroutine state is strictly nested within the lifetime of the caller, and
% the size of coroutine frame is known at the call site. In that case, coroutine state is embedded in the caller's stack frame (if the caller is an ordinary function) or coroutine state (if the caller is a coroutine).
% If allocation fails, the coroutine throws std::bad\_alloc, unless the Promise type defines the member function Promise::get\_return\_object\_on\_allocation\_failure(). If that member function is defined, allocation uses the nothrow form of operator new and on allocation failure, the coroutine immediately returns the object obtained from Promise::get\_return\_object\_on\_allocation\_failure() to the caller, e.g.:

%%% ------- Implementation details of a coroutine ------------------------
\subsection{Implementation details/concepts of a coroutine}\label{subsec:ImplementationOfACoroutine}
% \begin{lstlisting}[language=C, frame=single, caption={minimum coroutine co_yield example}, label={lst:minimum_coroutine_co_yield_example}]
%   struct Task {
%     struct promise_type {
%       int _val{};
%       Task get_return_object() { return Task{this}; }
%       std::suspend_never initial_suspend() { return {}; }
%       std::suspend_always final_suspend() noexcept { return {}; }
%       std::suspend_always yield_value(int value) { _val = value; return {}; }
%       void unhandled_exception() { }
%       };

%       std::coroutine_handle<promise_type> h{};  // coroutine handle

%       explicit Task(promise_type* p) : h{std::coroutine_handle<promise_type>::from_promise(*p)}{}
%       Task(Task&& rhs) : h{std::exchange(rhs.h, nullptr)} { }
%       ~Task() { if (h) { h.destroy(); } }
% };
% Task Coroutine() {
%     for (int i = 0; i < 5; ++i) {
%       co_yield i;
%     }
% }
% \end{lstlisting}
The following is a minimum coroutine example using co\_await, illustrated in \autoref{lst:minimum_coroutine_co_await_example}.
The Task struct is a the coroutine wrapper holding the coroutine handle and the promise\_type struct defining the promise type and thus, the behavior of the coroutine.
The function Task Coroutine(int num\_steps) is the coroutine function containing a co\_await expression suspending the coroutine for num\_steps times.
And as explained in previous subsection, also see \autoref{lst:coroutine_execution}, when another function, e.g. main(), calls the coroutine, the coroutine frame is allocated on the heap, the promise object is constructed and initial\_suspend() is called.
When the coroutine hits the co\_await expression, it suspends execution and returns control to the caller until it is resumed again.
The coroutine results in the output of "Coroutine at step i" and "Resuming coroutine" for num\_steps times, in this case from 0 to 4, as initial\_suspend() is set to suspend\_never and thus, the coroutine starts executing immediately when called.
If it it set to suspend\_always, the coroutine initially pauses before starting work (lazily computed coroutine) and results in "Resuming coroutine" being printed first followed by "Coroutine at step i"  num\_steps times.
\begin{lstlisting}[language=C, frame=single, caption={minimum coroutine co\_await example}, label={lst:minimum_coroutine_co_await_example}]
  struct Task {
    struct promise_type {
        Task get_return_object() { return Task{this}; }
        std::suspend_always initial_suspend() { return {}; }
        std::suspend_never final_suspend() noexcept { return {}; }
        void unhandled_exception() { }
        void return_void() { }
      };

      std::coroutine_handle<promise_type> h{};  // coroutine handle

      explicit Task(promise_type* p) : h{std::coroutine_handle<promise_type>::from_promise(*p)}{}
      Task(Task&& rhs) : h{std::exchange(rhs.h, nullptr)} { }
      ~Task() { if (h) { h.destroy(); } }
};
Task PrintCoroStep(int num_steps) {
    for (int i = 0; i < num_steps; ++i) {
      std::cout << "Coroutine at step " << i << "\n";
      co_await std::suspend_always{};  // if suspend_never, coro would never pause
    }
    co_return;  // equivalent to omitting this line
}
int main() {
    Task task = PrintCoroStep(5);
    for (int i = 0; i < 5; ++i) {
      std::cout << "Resuming coroutine\n";
      task.h.resume();
    }
}
\end{lstlisting}
In this simple example, trivial awaitables std::suspend\_always and std::suspend\_never, defined in the standard library, are used to control the suspension behavior of the coroutine at the initial and co\_await suspension points.
The implementation of suspend\_always is shown in \autoref{lst:suspend_always} returning false in await\_ready(), indicating that the await expression always suspends and waits for a value (is not ready).
The suspend\_never implementation is analog with the only difference of returning true in await\_ready(), indicating that the await expression never suspends (is always ready).
\begin{lstlisting}[language=C, frame=single, caption={suspend\_always implementation}, label={lst:suspend_always}]
struct suspend_always {
  constexpr bool await_ready() const noexcept{return false;}
  constexpr void await_suspend(std::coroutine_handle <>) const noexcept {}
  constexpr void await_resume() const noexcept {}
};
\end{lstlisting}

For more customization, more complex awaitabletypes can be implemented, requiring to implement these three methods listed in \autoref{lst:awaitable_type}.
\begin{lstlisting}[language=C, frame=single, caption={awaitable type}, label={lst:awaitable_type}]
  bool await_ready();

  // one of:
  void await_suspend(std::coroutine_handle<> h);
  bool await_suspend(std::coroutine_handle<> h);
  std::coroutine_handle<> await_suspend(std::coroutine_handle<> h);

  T await_resume();
\end{lstlisting}

In the $ co\_await expr; $ operator the expression is firstly converted to an awaitable type.
If the promise\_type type of the current coroutine has a member function await\_transform, then the awaitable is obtained by calling promise.await\_transform(expr), illustrated in \autoref{lst:pseudo_code_which_awaiter_awaitable}.
Otherwise, the awaitable is expr as-is.

\begin{lstlisting}[language=Python, frame=single, caption={Pseudo Code for deciding which awaiter/awaitable is used}, label={lst:pseudo_code_which_awaiter_awaitable}]
func get_awaitable(promise_type& promise, T&& expr){
  if P has member function await_transform:
    return promise.await_transform(expr);
  else
    return expr;
}

func get_awaiter(awaitable){
  if awaitable has member operator co_await:
    return awaitable.operator co_await();
  else if awaitable has non-member operator co_await:
    return operator co_await(awaitable);
  else:
    return awaitable;
}
\end{lstlisting}

Then, the awaiter object is obtained like illustrated in func get\_awaiter in \autoref{lst:pseudo_code_which_awaiter_awaitable}.
If no operator co\_await is defined, the awaitable itself is the awaiter which implicates that a type can be an awaitable and an awaiter type simultaneously.
For the simple example, in \autoref{lst:minimum_coroutine_co_await_example}, the expression std::suspend\_always after the co\_await is the expression which already
is the awaitable as-is as it is an awaitable and has no await\_transform() member. The awaiter is also std::suspend\_always as it has no operator co\_await defined.
Thus, std::suspend\_always is both the awaitable and the awaiter type.

Then, awaiter.await\_ready() is called , The coroutine is suspended awaiter.await\_suspend(handle) is called, where handle is the coroutine handle representing the current coroutine.
Inside that function, the suspended coroutine state is observable via that handle,
if await\_suspend returns void, control is immediately returned to the caller/resumer of the current coroutine (this coroutine remains suspended), otherwise
if await\_suspend returns bool, the value true returns control to the caller/resumer of the current coroutine the value false resumes the current coroutine.
if await\_suspend returns a coroutine handle for some other coroutine, that handle is resumed (by a call to handle.resume())
Finally, awaiter.await\_resume() is called (whether the coroutine was suspended or not), and its result is the result of the whole co\_await expr expression.

If the coroutine was suspended in the co\_await expression, and is later resumed, the resume point is immediately before the call to awaiter.await\_resume().

Additionally to suspending the coroutine, the co\_yield expression returns a value to the caller and is
equivalent to co\_await promise.yield\_value(expr).
However, to define the type to return, the promise\_type has to implement the yield\_value() function.


Besides the awaiter/awaitable concept explained in the previous paragraphs, there is also the promise type concept.
These are the two main interfaces, defined by the coroutine Type Specification (TS), for customizing the behavior of coroutines and co\_await expressions:
\begin{itemize}
  \item Awaiter / Awaitable: specifies methods that controls the semantics of co\_await expression. When a value is co\_awaited, the awaitable object allows to specify whether to suspend,
   execute some logic after suspensions (for asynchronously completed operations) and/or execute some logic after the coroutine resumes.
  \item Promise Type: specifies methods for customising the behavior of a coroutine, e.g. the behavior of any co\_await or co\_yield expression inside the coroutine body.
\end{itemize}


The promise\_type struct has to be implemented in the coroutine wrapper type, which can be named arbitrarily, e.g. Task in \autoref{lst:minimum_coroutine_co_await_example}.
It has to follow the scheme illustrated in \autoref{lst:promise_type}.
% A coroutine in C++ is an finite state machine (FSM) that can be controlled and customized by the promise_type.
\begin{lstlisting}[language=C, frame=single, caption={Promise Type}, label={lst:promise_type}]
struct promise_type{
  // required methods
  ReturnType get_return_object();
  std::suspend_always initial_suspend();
  std::suspend_always final_suspend() noexcept;
  void unhandled_exception();

  // depending on ReturnType
  void return_void(); // if ReturnType is void
  void return_value(T value); // if ReturnType is T

  // optional methods
  auto await_transform(U&& value); // customize co_await behavior
  auto yield_value(V value); // customize co_yield behavior
}
\end{lstlisting}

The Promise type is determined by the compiler from the return type of the coroutine using std::coroutine\_traits.

As mentioned in \autoref{subsec:ExecutionOfACoroutine}, each coroutine is associated with a coroutine handle.
This handle is a non-owning reference to the coroutine frame and is used to resume execution of the coroutine or to destroy the coroutine frame.

% \begin{lstlisting}[language=C, frame=single, caption={coroutine chat with await\_transform example}, label={lst:coroutine_chat_await_transform_example}]
% // #include <coroutine> #include <iostream> #include <utility>
% struct Chat {
%     struct promise_type {
%         std::string _msgOut{}, _msgIn{};

%         void unhandled_exception()noexcept{};
%         Chat get_return_object() {return Chat{this};}
%         std::suspend_always initial_suspend() { return {}; }
%         std::suspend_always final_suspend() noexcept { return {}; }
%         std::suspend_always yield_value(std::string msg) {
%           _msgOut = std::move(msg); return {};}
%         void return_value(std::string msg) noexcept {
%           _msgOut = std::move(msg); }
%         auto await_transform(std::string) noexcept {
%             struct awaiter {
%                 promise_type& pt;
%                 constexpr bool await_ready() const noexcept {return true;}
%                 void await_suspend(std::coroutine_handle<>)const noexcept{}
%                 std::string await_resume() const noexcept {
%                   return std::move(pt._msgIn); }
%             };
%             return awaiter{*this};
%         }
%     };
%     std::coroutine_handle<promise_type> _hdl;
%     explicit Chat(promise_type* p) : _hdl{
%       std::coroutine_handle<promise_type>::from_promise(*p)}{}
%     Chat(Chat&& rhs) : _hdl{std::exchange(rhs._hdl, nullptr)} {}
%     ~Chat() {if (_hdl) {_hdl.destroy();}}

%     std::string listen() {
%         if(not _hdl.done()) {_hdl.resume();}
%         return std::move(_hdl.promise()._msgOut);}
%     void answer(std::string msg) {
%         _hdl.promise()._msgIn = msg;
%         if(not _hdl.done()) {_hdl.resume();}}
% };

% Chat Fun(){
%     co_yield "Hello\n";
%     std::cout << co_await std::string{};
%     co_return "Here!\n";}
% int main(){
%     Chat chat = Fun();
%     std::cout << chat.listen();
%     chat.answer("Where are you?\n");
%     std::cout << chat.listen();
% }
% \end{lstlisting}


\cite{baker2018understanding}
\cite{baker2018promise}


% %%% ------ Comparison to other alternative solutions to hide cache misses ------------------------
% \subsection{Comparison to other alternative solutions to hide cache misses}
% AMAC, Group Prefetching
% coro is better, not performance wise but easier to use, more general


%%% ====== CACHE MISSES =======================
\section{Cache Misses and Software Prefetching}\label{section:CacheMisses}
This section provides a background on cache misses, why they cause latencies in modern computer architectures by accessing data
and how they can be potentially be hidden.

\subsection{Definition of Cache Misses}\label{subsec:DefinitionOfCacheMisses}

% CACHE MISSES von wikipedia, data misses, write misses, read misses
To explain cache misses, first the underlying concept of caches in modern computer architectures has to be explained briefly.
Caches are small, fast memory units located close to the CPU cores, designed to store frequently accessed data and instructions to reduce the average time to access memory.
When the CPU needs to read or write data, it first checks if the data is present in the cache (a cache hit).
Typically, there are three levels of caches (L1, L2, L3 or called LLC) with L1 being the smallest and fastest, and L3 being the largest and slowest.
If the data is not found in the caches (a cache miss), it has to be fetched from the slower main memory.

As described, a cache miss is a failed attempt to read or write a piece of data in the cache, which results in a main memory access with much longer latency.
Data is transferred between memory and cache in fixed-size blocks called cache lines.
When a cache line is requested, the cache checks if it contains the data at the requested memory location (a cache hit).
If not (a cache miss), the cache allocates a new entry and fetches the data from main memory.
Cache misses cause stalls because the CPU must wait for data to arrive from the slower main memory. Modern CPUs can execute hundreds of instructions in the time it takes to fetch a single cache line, making these stalls particularly costly. To mitigate this, modern architectures employ techniques such as out-of-order execution, which allows the CPU to execute independent instructions while waiting for cache miss data, and simultaneous multithreading (SMT), which enables an alternate thread to utilize the CPU core during the stall.
There are three types of cache misses: instruction read miss, data read miss, and data write miss
Instruction cache misses typically incur the largest penalty, as the processor must stall until the instruction is fetched from main memory.
Data cache read misses cause smaller delays because independent instructions can continue execution while waiting for data and
Data cache write misses result even in the shortest delays, as writes can be queued without blocking subsequent instructions.

In this work, the focus lies on data read misses, as they are the most common type of cache misses encountered during data structure traversals.
These data read cache misses are typically categorized into three main types: compulsory misses, capacity misses, and conflict misses.
Compulsory misses occur on the first access to a block, as the data has not yet been loaded into the cache regardless of cache design.
Capacity misses happen when the cache can not hold all the data actively used by a program during execution, meaning the working set exceeds the cache size.
Conflict misses, also known as interference misses, occur when multiple memory addresses map to the same cache set, leading to evictions of useful data even though the cache is not full.
For a multi-processor system this 3Cs group of cache misses can be extended to 4Cs.
The 4th C are coherence misses arising from a cache line being invalidated due to modifications made by other processors (for L3 Cache as it is typically shared across cores) or threads (for L1, L2 and L3 Cache).

In this work, the focus lies merely on the first two Cs, compulsory and capacity misses,
as the first are inherent to the initial data access and the second can be mitigated through better data locality and prefetching techniques.

%  Computer Organization and Design MIPS Edition 5th Edition The Hardware/Software Interface

\subsection{Cause of Cache Misses in Data Structures}\label{subsec:CauseOfCacheMissesInDataStructures}

Modern database engines utilize various in-memory data structures that are directly addressed by virtual memory pointers.
Memory blocks used by these data structures are typically allocated from the heap and chained together using pointers.
For example, the nodes of an in-memory B+-tree are dynamically allocated and deallocated as the tree grows and shrinks.
To traverse such a tree from its root node to the target leaf node,
the accessing thread must dereference multiple pointers sequentially from the root to the leaf, forming a random access pattern.
Another example is a chaining Hashtable, where each bucket contains a linked list of entries that hash to the same index.
To look up a key in such a hashtable, the accessing thread first computes the hash index, accesses the corresponding bucket,
and then traverses the linked list by following pointers until it finds the desired key or reaches the end of the list.

This pointer chasing behavior poses a significant challenge for hardware prefetchers.
Such access patterns are very difficult, if not impossible, for hardware prefetchers to predict accurately,
leading to a high likelihood of last-level cache misses upon each pointer dereference.
But this pointer chasing is also for software prefetching challenging, as the address of the next memory access is not known until the current access is completed
and the node or linked list entry is fetched and processed to extract the next pointer.

% TODO: make my example also with hashtable and pictures, rather in next chapter?

\subsection{Techniques to counter Cache Misses}\label{subsec:TechniquesCounterCacheMisses}

The literature presents various software techniques to address cache misses.
Based on their impact on the number of cache misses and the incurred penalty, these techniques can be categorized into three main approaches.

\begin{itemize}
  \item Eliminate cache misses by increasing spatial and temporal locality.
  \item Reduce the cache miss penalty by scheduling independent instructions to execute after a load.
  \item Hide the cache miss penalty by overlapping memory accesses.
\end{itemize}

The first approach improves cache locality by either eliminating indirection by designing cache-conscious data structures,
matching the data layout to the access pattern of the algorithm such that data accessed together is stored in contiguous space,
or reorganizing memory accesses. However, these optimizations often require fundamental changes to data structure designs.

The second approach focuses on reducing the cache miss penalty by scheduling independent instructions to execute after a memory load.
This increases instruction-level parallelism and leads to more effective out-of-order execution.
Finally, to reduce the main-memory access penalty, a non-blocking load, a prefetch instruction, must be introduced early enough to allow independent instructions to execute while fetching data.
However, this approach relies on the presence of independent instructions following the load and timing it correctly to ensure that the data is available when needed.

The third approach seeks to hide the cache miss penalty by overlapping memory accesses, meaning issuing multiple memory requests in parallel.
This, however, requires independent memory accesses which are not naturally available in dependent access patterns like index lookups.

These traditional approaches provide limited benefits for individual index lookups with dependent memory accesses if parallelism cannot be exploited.
To effectively hide cache misses in such scenarios, techniques that can create parallelism across multiple independent operations are needed,
such as processing operations in groups with interleaved execution.

In the literature two main techniques have been proposed to achieve this, :
\begin{itemize}
  \item Group Prefetching (GP)
  \item Asynchronous Memory Access Chaining (AMAC)
  \item Coroutines
\end{itemize}

Group Prefetching (GP) interleaves N identical operations across M predefined code stages, executing each operation through all stages sequentially before moving to the next stage.
While GP maintains a "semi-synchronous" programming model, it has significant limitations:
it requires knowing the number of stages in advance, and cannot efficiently handle early-terminating operations, leading to performance degradation on irregular access patterns.

Asynchronous Memory Access Chaining (AMAC) transforms operations into state machines stored in a circular buffer,
allowing different operations to execute different stages concurrently. This enables immediate reuse of completed operations without waiting for others.
However, AMAC requires complete code rewriting, resulting in code that is difficult to understand and maintain.

Both GP and AMAC achieve high performance through careful hand-coding, but at the cost of developer productivity.
Coroutines offer an alternative that combines the benefits of both approaches: they enable interleaved execution like AMAC while preserving the synchronous programming model like GP,
but with compiler-generated context-switching code rather than manual state machine implementation. Developers simply add suspension points (co\_await or co\_yield) after prefetch operations,
and the compiler handles the efficient state management and context switching.

All these techniques rely on using software prefetching to initiate memory transfers ahead of time which is explained in the next subsection.

% Group prefetching and software pipelined prefetching [5] overlap multiple hash table
% lookups to accelerate hash joins. After a prefetch is issued, the
% control flow switches to execute the computation stage of another
% operation. Asynchronous memory access chaining (AMAC) [26] is
% a general approach that allows one to transform a heterogeneous
% set of operations into state machines to facilitate switching between operations upon cache misses. A notable drawback of these
% approaches is they require developers hand-craft algorithms. The
% resulted code is typically not intuitive to understand and hard to
% maintain. This limits their application to simple or individual data
% structures and operations (e.g., tree traversal). or coroutines.

% Consider traversing a tree-like data structure: the immediate succession of identifying
% and accessing a node leaves negligible time for the hardware to load the data, even with prefetch hints.
% To create a sufficient temporal gap between pinpointing the next node and accessing it, various methods have been explored.
% The key approach is to break operations into stages and use pipelining, overlaying the prefetch of one item with the compute of others, for example, by processing operations in groups.
% A more general approach utilizes asynchronous control flow abstractions, such as coroutines and fine-grained tasks.

\subsection{Software Prefetching}\label{subsec:SoftwarePrefetching}
Hardware manufacturers have implemented various hardware prefetching techniques to automatically detect and pre-load data into caches before they are accessed by the CPU.
These hardware prefetchers monitor memory access patterns and attempt to predict future accesses based on historical data.
However, the reliance on hardware prefetchers has limitations, especially for applications with irregular access patterns where predictions may be inaccurate or insufficient.

The solution to overcome this is Software-based prefetching.
It enables applications to proactively hint to the CPU about upcoming data accesses which for the hardware is hard or even impossible to predict.
These hints, specific instrucctions in modern ISAs such as x86 and ARMv8, allow the hardware to move data asynchronously into the cache hierarchy,
thereby reducing access latency by overlapping data transfers with computation.

With Software prefetching, proactively hinting the CPU what data to preload into the cache, timing of these prefetches is crucial for effectiveness.
When software executes the prefetching instruction too early before the actual access, the data may have already been evicted from the cache when needed.
Conversely, if prefetched too late, the data might not be transferred to the cache completely, potentially causing the CPU to stall.
The presence of intricate memory systems like NUMA and high-bandwidth memory,
as well as the increasing sophistication of CPUs with out-of-order execution and superscalar architectures, further complicate achieving precise timing.

Before diving into the details of software prefetching, it is essential to understand the data load process through the memory subsystem of modern computer architectures.
As illustrated in \autoref{fig:DataLoadThroughMemorySubsystem}, when the CPU executes a load instruction to read data from memory,
it first translates the given virtual address into a physical address using the Memory Management Unit (MMU), in particular the Translation Lookaside Buffer (TLB).
If the TLB does not contain the mapping for the virtual address, a page-table walk is initiated to retrieve the mapping from the page tables in memory and to finally update the TLB.
Once the physical address is determined, the CPU checks the cache hierarchy (L1, L2, L3) for the requested data.
If the data is found in any of the cache levels (a cache hit), it is returned to the CPU.
However, if the data is not present in the caches (a cache miss), the CPU issues a request to the main memory to fetch the data.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\paperwidth]{figures/sequenzdiagramm_data_load_final.drawio.png}
  \caption{Data Load through Memory Subsystem}
  % \quelle{\cite{ADaSurvey}}
  \label{fig:DataLoadThroughMemorySubsystem}
\end{figure}

% Lifecycle of a Software Prefetch
Back to pretetching, modern platforms offer several prefetch instructions to target different levels of the cache hierarchy, with each instruction initiating the transfer of one cache line.
As visualized in \autoref{fig:PrefetchLifecycleThroughMemorySubsystem}, when the software executes a prefetch, the logical address is first translated into a physical one.
This procedure happens synchronously—if the translation is not cached within the Translation Lookaside Buffer (TLB),
the CPU contacts a Page Miss Handler (PMH) before continuing execution of the prefetch instruction.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\paperwidth]{figures/prefetch_instructions_lifecycle_through_memory_subsystem.png}
  \caption{Lifecycle of prefetch instructions through the memory subsystem}
  % \quelle{\cite{ADaSurvey}}
  \label{fig:PrefetchLifecycleThroughMemorySubsystem}
\end{figure}

Once the physical address is known and the request misses the L1 data cache (L1d), the hardware triggers the transfer from memory to caches asynchronously.
The L1d cache requests the cache line through the Line Fill Buffer (LFB), which acts as a buffer-like structure interfacing between the L1d and L2 cache to communicate requests on a cache-line granularity.
For every cache line that misses the L1d, the CPU allocates a slot in the LFB.
This nature makes it straightforward to implement asynchronous prefetches: the desired cache line does not cause the instruction to wait until the data is transferred into the cache but only until the miss is communicated to the LFB.
Furthermore, the LFB enables the CPU to handle several pending requests simultaneously without blocking, supporting out-of-order execution and performing micro-optimizations like merging multiple requests to the same cache line.

The specific software prefetch instruction dictates how close the data is moved to the CPU.
In the x86 ISA, most prefetch instructions clearly specify the cache level target for the fetched data:
\texttt{prefetcht1} stores cache lines in the L2 cache, \texttt{prefetcht0} moves data into the L1d, and \texttt{prefetcht2} is generally aimed at the LLC (though on recent Intel platforms since Skylake with non-inclusive victim caches, data is placed into the L2 instead).
The non-temporal \texttt{prefetchnta} instruction does not specify a particular cache target but aims to reduce cache pollution for data that will be accessed non-recurringly, with these cache lines being prioritized for quicker eviction.

% Limitations
Modern platforms exhibit two notable limitations in the implementation of software prefetches.
First, the execution of a single prefetch instruction stalls until the virtual address is translated into a physical address.
Although the TLB might accelerate this process, prefetched addresses are often not present in the TLB since software prefetching is primarily used when the application accesses scattered data objects.
Consequently, the latency of the prefetch instruction is dominated by the latency of the PMH performing a page-table walk to translate the address.
However, this restriction only applies to the initial prefetch operation within a memory page if several prefetches are performed consecutively.

Second, due to the limited number of LFB entries (typically between 3 and 24), the hardware can accommodate only a small number of outstanding memory requests.
If an excessive volume of prefetch requests is issued rapidly, the LFB becomes saturated and cannot accept further requests.
Under this circumstance, the CPU will either stall until an LFB slot becomes available or drop new prefetch requests.
This limitation necessitates careful management of prefetch intensity to balance memory-level parallelism with hardware constraints.

Thus, to efficiently exploit software prefetching in data structures like trees and hash tables, these prefetch instructions must be strategically placed within the code
to have the data in cache when needed. As mentioned in previous chapter, techniques such as breaking operations into stages and pipelining,
or utilizing asynchronous control flow abstractions like coroutines, can facilitate effective prefetching by creating sufficient temporal gaps between prefetch issuance and data access.


% https://dl.acm.org/doi/epdf/10.1145/2133382.2133384


% %%% ====== COMPUTER ARCHITECTURE =======================
% \section{Computer Architecture}\label{section:ComputerArchitecture}

% \subsection{Different Computer Architectures, x86-64, amd, apple, mips}

% \subsection{Introduction to computer achitecture and storage layout}
% pyramid, and tlb access schemata

% \subsection{Different Compilers - Gnu vs Clang}

% % There are multiple C compilers due to a combination of historical development, hardware and software specialization, and differing design philosophies. The need for compilers optimized for specific hardware and operating systems has driven the creation of diverse tools. For example, Intel's ICC (Intel C Compiler) is optimized for Intel processors, while AMD's AOCC (AMD Optimizing C/C++ Compiler) is tailored for AMD systems, aiming to maximize performance on their respective architectures.
% %  Similarly, compilers like ACK were developed specifically for operating systems such as OpenBSD, highlighting the role of the target environment in compiler design.

% % The evolution of the C language itself, with multiple standards (C89, C99, C11, C17, C23), has also contributed to diversity. Some compilers support newer language features more comprehensively than others, and certain compilers may not support all C standards equally.
% %  This leads to a need for multiple compilers to ensure code compatibility and performance across different platforms and use cases.

% % Additionally, the design and architecture of compilers vary significantly. GCC (GNU Compiler Collection) is a monolithic system with a complex internal structure, while Clang, part of the LLVM project, was designed from the start as a modular API, making it easier to integrate into tools like IDEs, static analyzers, and refactoring utilities.
% %  This architectural difference influences how compilers are used and extended.

% % Despite the existence of many compilers, only a few are widely used in practice. The major players include GCC, Clang, and Microsoft Visual C++ (MSVC), which dominate due to their robustness, extensive feature sets, and strong community support.
% %  However, niche compilers like Pelles C, LCC, and TCC (Tiny C Compiler) exist for specific purposes such as embedded development or minimal footprint requirements.
% %  Some compilers, like the Portable C Compiler or LabWindows/CVI, are used by specialized developer communities.

% % The presence of multiple compilers also stems from the fact that C and C++ are closely related, with C++ being largely a superset of C. As a result, many compilers that support C++ also support C, and the effort to build a C++ compiler often includes C support as a natural byproduct.
% %  This shared foundation reduces the marginal cost of supporting both languages, but it does not eliminate the need for specialized tools optimized for particular environments or performance goals.

% % file://wsl.localhost/Ubuntu/home/danieldagruber/TUM_Master_11/00_MasterThesis/07_p577-huang_TheArtOfLatencyHidingInModernDatabaseEngines.pdf
% % where can latencies come from: pointer chasing, synch, storage io, oversubscription, osschedling
% % https://chipsandcheese.com/p/skylake-intels-longest-serving-architecture
